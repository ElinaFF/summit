Classification on Plausible database for ViewNumber0 with AdaboostPregen.

f1_score on train : 0.8799999999999999
f1_score on test : 0.8571428571428571

Database configuration : 
	- Database name : Plausible
	- View name : ViewNumber0	 View shape : (34, 10)
	- Learning Rate : 0.7941176470588235
	- Labels used : No, Yes
	- Number of cross validation folds : 5

Classifier configuration : 
	- AdaboostPregenwith n_estimators : 130, base_estimator : DecisionTreeClassifier, n_stumps : 1, random_state : <mtrand.RandomState object at 0x7fc8ceb8ca68>
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.8799999999999999
		- Score on test : 0.8571428571428571
	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.8888888888888888
		- Score on test : 0.8571428571428571
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.8799999999999999
		- Score on test : 0.8571428571428571
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.1111111111111111
		- Score on test : 0.14285714285714285
	For Jaccard_similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.8888888888888888
		- Score on test : 0.8571428571428571
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.9166666666666666
		- Score on test : 1.0
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binaryas average (higher is better)  : 
		- Score on train : 0.8461538461538461
		- Score on test : 0.75
	For Zero_one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.11111111111111116
		- Score on test : 0.1428571428571429


 Classification took 0:00:15

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.3013127912156881
- Feature index : 17, feature importance : 0.15151455245406334
- Feature index : 2, feature importance : 0.09896909766512689
- Feature index : 19, feature importance : 0.06305256140479262
- Feature index : 14, feature importance : 0.051780205671756584
- Feature index : 16, feature importance : 0.05100864156571934
- Feature index : 4, feature importance : 0.048427791586712815
- Feature index : 6, feature importance : 0.04663002189656192
- Feature index : 12, feature importance : 0.040279845407527215
- Feature index : 11, feature importance : 0.031614704971605824
- Feature index : 5, feature importance : 0.027590872063692262
- Feature index : 9, feature importance : 0.02097031950719099
- Feature index : 13, feature importance : 0.02069167788784662
- Feature index : 1, feature importance : 0.019495590716871788
- Feature index : 3, feature importance : 0.016835249803455413
- Feature index : 15, feature importance : 0.009826076181388222


 Estimator error | Estimator weight
0.14814814814814817 | 0.10440711755432026
0.41847826086956513 | 0.01963891434352781
0.3811142128899139 | 0.02893827714766852
0.4170436816011132 | 0.019990949782974818
0.4243148571771182 | 0.018210073350420318
0.4469235923129273 | 0.01272012789234389
0.45217240892772315 | 0.011454049071087559
0.43824970122956036 | 0.014818780668970641
0.443431872408569 | 0.013563937817612781
0.4491813088217686 | 0.01217520607781685
0.44374382634613113 | 0.01348849725591584
0.4494332141211665 | 0.012114438041911667
0.4527955993545187 | 0.011303903867641964
0.4599435763098383 | 0.009584170731643787
0.4588574434196328 | 0.009845210521063902
0.4506058251055844 | 0.011831646632135614
0.4550466885602598 | 0.010761842607952554
0.4587548965240934 | 0.009869861326047068
0.45682419168759375 | 0.010334134253186326
0.4602561383886469 | 0.009509067014230859
0.4611205988439002 | 0.009301389784497212
0.4651948219612484 | 0.008323336912557667
0.46525730289987965 | 0.008308346832605425
0.4598292035118176 | 0.009611654592625106
0.4628165788030886 | 0.008894111045194018
0.4653903864772463 | 0.008276419030847137
0.4627695835193437 | 0.008905393834931821
0.4653496755409447 | 0.008286185780428543
0.4664526992972881 | 0.00802160378343853
0.4685620181579701 | 0.007515858890992455
0.46701510048547173 | 0.007886731215119396
0.4681071422598583 | 0.00762489968119632
0.47002347406467554 | 0.007165609135923521
0.4673950861940193 | 0.0077956161217609884
0.46800478005474017 | 0.007649439230423226
0.46992903437313605 | 0.007188238585576693
0.467903011656256 | 0.0076738370648875495
0.4698391561624392 | 0.007209775501517522
0.47075553943281057 | 0.006990210420601013
0.4723715005577497 | 0.006603140900053015
0.47103463251360733 | 0.006923349276904305
0.47262073354250556 | 0.006543454924541987
0.47140062191160764 | 0.006835677319958663
0.4710784936556178 | 0.006912842045015272
0.4741729743102686 | 0.006171798798350427
0.47300024622318454 | 0.006452576011791595
0.47187604892607204 | 0.006721800805868117
0.47337372124788263 | 0.006363150183365279
0.47388887756707093 | 0.006239811504457398
0.47518478386069674 | 0.00592960399109178
0.47375282203461005 | 0.00627238460350718
0.47506192995954066 | 0.005959008640084392
0.4738068617175641 | 0.006259446834086042
0.4751107185016384 | 0.005947331187900488
0.47382196132601645 | 0.006255831827074605
0.4751243526431142 | 0.005944067900334611
0.47403494931073115 | 0.006204841454193466
0.4753167528381952 | 0.0058980184814533265
0.47586020468333967 | 0.0057679571383252495
0.4769719876907304 | 0.005501922508054545
0.47576560104234955 | 0.005790597115496956
0.47688591305164596 | 0.005522517020522513
0.47567098634920596 | 0.005813240153905141
0.4762920688860356 | 0.005664611223348813
0.4831757996142457 | 0.004018362224618078
0.47457510026795674 | 0.006075536753070652
0.47643399226927435 | 0.005630650550851302
0.4783747825300203 | 0.005166330259571772
0.4756138710891259 | 0.005826909089630615
0.47483715438498164 | 0.006012809755841216
0.47604281621870054 | 0.005724256809659673
0.4749467356226924 | 0.005986580655587759
0.4761421687311859 | 0.005700481642770252
0.4760996339287682 | 0.005710660212934492
0.4771899700600875 | 0.0054497687620029284
0.47601713449159466 | 0.005730402549288945
0.47711483801561383 | 0.005467744372514129
0.4763161551952339 | 0.005658847575529875
0.47738726806285614 | 0.005402565682438499
0.4767678182005347 | 0.005550773333182029
0.4777993569512804 | 0.00530397982051103
0.4768336685068739 | 0.005535017389071407
0.47785949697201585 | 0.005289592862972084
0.47781246010375433 | 0.0053008452142359885
0.4787551998074733 | 0.005075336875594752
0.477756425603061 | 0.005314250138669516
0.4787038314232733 | 0.005087623518600173
0.477704830431542 | 0.0053265931773821525
0.47865654244239025 | 0.005098934517121916
0.4783297018168477 | 0.005177113673908479
0.47922989073882044 | 0.0049618020886108465
0.4786054547863808 | 0.005111154219328329
0.47807726177607024 | 0.005237499500238692
0.47989217614473206 | 0.004803414105257214
0.4794794457709981 | 0.004902118067642912
0.47873705024511615 | 0.005079678000306582
0.47960439182865156 | 0.0048722366704464825
0.4792853850054539 | 0.004948529768776767
0.4801094357657283 | 0.0047514593790169415
0.47924762331333787 | 0.004957561053391903
0.4800746212444569 | 0.0047597846835756965
0.4792687801037484 | 0.004952501077299584
0.48009412619778963 | 0.00475512039783387
0.48005598676121947 | 0.004764240824413915
0.48041216648763263 | 0.004679068380066363
0.48709552228715725 | 0.003081680091001124
0.4790853918221246 | 0.004996361835435067
0.47992510878987105 | 0.004795538588136306
0.47958218948884834 | 0.004877546412611457
0.4803832515925067 | 0.0046859825541531374
0.4795499503059465 | 0.004885256523926777
0.48035349434006674 | 0.004693098186823328
0.4797254883562816 | 0.004843276487545766
0.48051556323635336 | 0.004654344247248808
0.4802874261776679 | 0.0047088966992594175
0.4810351193955614 | 0.004530114467272346
0.48053848422962636 | 0.004648863455118582
0.4812676057999102 | 0.004474528413726416
0.4807049140057285 | 0.004609067903345653
0.48097451125645446 | 0.0045446058181198646
0.4821473054793186 | 0.004264215039990146
0.4812192155318723 | 0.004486098065274138
0.48086162158173507 | 0.004571597993739137
0.481567170513788 | 0.004402907209426532
0.48084122167563864 | 0.004576475707020767
0.48154824773819815 | 0.004407431252713584
0.48114718269018963 | 0.004503320586770262
0.48183221071482296 | 0.004339542915408617
0.4817315600195529 | 0.004363605617668583
0.482375504110248 | 0.004209663041216219