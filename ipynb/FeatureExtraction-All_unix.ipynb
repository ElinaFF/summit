{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Extract ColorHistograms for Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Nikolas HÃ¼lsmann\n",
    "#### Date: 2015-11-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Extract Data\n",
    "\n",
    "### Function to iterate through given directory and return images paths and classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imgCrawl(path, sClassLabels): #path to 'highest' folder\n",
    "    rootdir = path\n",
    "    df = pd.DataFrame()\n",
    "        \n",
    "    for subdir, dirs, files in os.walk(rootdir): # loop through subdirectories\n",
    "        for file in files:\n",
    "            pathOfFile = os.path.join(subdir, file) #path of file\n",
    "            head, classLabel = os.path.split(os.path.split(pathOfFile)[0]) # get directoryname of file as classLabel\n",
    "            \n",
    "            # assign integer label for dataframe\n",
    "            classLabel = sClassLabels[sClassLabels == classLabel].index[0]\n",
    "            df = df.append({'classLabel': classLabel, 'pathOfFile': pathOfFile}, ignore_index=True) \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to determine Class-Labels with Integer representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to determine Class-labels and return Series\n",
    "def getClassLabels(path):\n",
    "    data = os.listdir(path) # listdir returns all subdirectories\n",
    "    index = range(0,len(data))\n",
    "    \n",
    "    return pd.Series(data,index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate the ColorHistogram for given Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Calculate ColorHistograms for all images\n",
    "\n",
    "### Points to improve: \n",
    "# - use HSV color spectrum\n",
    "# - change function: parameter how many bins of ColorHistogramm (feature length)\n",
    "\n",
    "\n",
    "# dfImages: Dataframe with paths to all images - use function imgCrawl\n",
    "# numberOfBins_: Number of bins Histogram\n",
    "def calcColorHisto(dfImages_, numberOfBins_):\n",
    "    # Initialize function\n",
    "    df = pd.DataFrame()\n",
    "    npImages = dfImages_.values\n",
    "    numberOfBins = numberOfBins_\n",
    "    npColorHist = np.zeros((len(npImages), numberOfBins*3), \"float32\")\n",
    "    i=0\n",
    "    \n",
    "    ## algo\n",
    "    for images in npImages:\n",
    "        image = cv2.imread(images[1])\n",
    "        \n",
    "        # Image Size for Normalization\n",
    "        height, width, channels = image.shape\n",
    "        img_size = height * width\n",
    "        \n",
    "        # Split into color chanels rgb\n",
    "        chans = cv2.split(image)\n",
    "        colors = (\"b\", \"g\", \"r\")\n",
    "        \n",
    "        histogram = []\n",
    "\n",
    "        ########### Feature Color Histogram (cf. http://docs.opencv.org/2.4/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.html)     # loop over the image channels\n",
    "        for (chan, color) in zip(chans, colors):         \n",
    "            \n",
    "            # Calculate Color Histogram - 16 bins cf. paper (test with 64 has shown that die result is similair in score)\n",
    "            # Seperates the intesity for each color from 0 to 256, and creates 16 bins of same size: 0-15, 16-31, .. , 241-256\n",
    "            hist = cv2.calcHist([chan], [0], None, [numberOfBins], [0, 256])\n",
    "\n",
    "            # to get raw values\n",
    "            hist = hist[:,0]\n",
    "            \n",
    "            # Normalize to a Distrubution from 0 to 1 throug calculating for each color channel (red/blue/green): \n",
    "            #        (number of pixels in bin)/(pixel size of image)\n",
    "            #hist[:] = [x / img_size for x in hist]\n",
    "            hist[:] = [x / sum(hist) for x in hist]\n",
    "            \n",
    "\n",
    "            # Normalize with MinMax from 0 to 1 -> feature scaling\n",
    "            #cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n",
    "            \n",
    "            histogram.extend(hist)\n",
    "\n",
    "        # append features_colHist to df\n",
    "        npColorHist[i] = histogram\n",
    "        i = i+1\n",
    "        #df = df.append({'ColHisto': features_colHist}, ignore_index=True) \n",
    "    \n",
    "    return npColorHist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate Surf Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################# FEATURE SURF (cf. http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html#surf)\n",
    "# API cf. http://docs.opencv.org/2.4/modules/nonfree/doc/feature_detection.html\n",
    "\n",
    "#### Calculate Histogramm of SURF Descripteurs with Bag Of Words appraoch for all images\n",
    "\n",
    "### Points to improve: \n",
    "# - use spatial histogram: http://www.di.ens.fr/willow/events/cvml2011/materials/practical-classification/\n",
    "# - change function: parameter how many K clustes/feature length (in regard of overfitting)\n",
    "\n",
    "\n",
    "# path to highest folder\n",
    "# dfImages: Dataframe with paths to all images - use function imgCrawl\n",
    "# k: number of K-Cluster -> length of feature vector\n",
    "def calcSurfHisto(dfImages_, k_):\n",
    "    \n",
    "    # Initialize function\n",
    "    df = pd.DataFrame()\n",
    "    npImages = dfImages_.values\n",
    "    k = k_\n",
    "    \n",
    "    # List where all the descriptors are stored\n",
    "    des_list = []\n",
    "    \n",
    "    #### Feature Detection and Description (Surf): \n",
    "    # Detect (localize) for each image the keypoints (Points of Interest in an image - Surf uses therefore like SIFT corners)\n",
    "    # Pro: SIFT/SURF are scale and rotation invariant!\n",
    "    for images in npImages:\n",
    "        # Read image\n",
    "        image = cv2.imread(images[1])\n",
    "        \n",
    "        # Method to detect keypoints (kp) and calculate the descripteurs (des) with one function call\n",
    "        # Each image has different amount of kp, but each kp has a describteur of fixed length (128)\n",
    "        kp, des = sift.detectAndCompute(image,None)\n",
    "        des_list.append(des)\n",
    "    \n",
    "    # Stack all the descriptors vertically in a numpy array\n",
    "    descriptors = des_list[0][1]\n",
    "    \n",
    "    for descriptor in des_list[0:]:\n",
    "        \n",
    "        descriptors = np.vstack((descriptors, descriptor)) \n",
    "    \n",
    "    #### Bag of Words Approach\n",
    "    ### 1. Step: using K-means cluster to create dictionary/vocabulary/codebook:\n",
    "    # Encoding is the quantization of the image kp/des that constitute the image to be classified. \n",
    "    # Basic encoding schemes work by first running K-means on the set of all des that you collect \n",
    "    # across multiple images.\n",
    "    # This builds what is known a dictionary/vocabulary/codebook represented by the centroids obtained from the clustering.\n",
    "    \n",
    "    # Perform k-means clustering -> creates the words from all describteurs -> this is the (dic) dictionary/vocabulary/codebook\n",
    "    # k: amount of different clusters to build! Will result in a feature length k\n",
    "    dic, variance = kmeans(descriptors, k, 1) \n",
    "    \n",
    "    ### 2. Step: encoding/coding/vector quantization(vq) to assign each descriptor the closest \"visual word\" from dictionary:\n",
    "    # At the end of this process, you end up with K representative \"visual words\" (the centroid of each cluster after \n",
    "    # K means ends) of your image descripteurs. These \"visual words\" represent what is usually understood as your \n",
    "    # visual dictionary. Once you have these visual words, encoding is the process of assigning \n",
    "    # each descriptor within your image the \"visual word\" (nearest neighbor) in the dictionary.\n",
    "    \n",
    "    npSurfHist = np.zeros((len(npImages), k), \"float32\")\n",
    "    for i in xrange(len(npImages)):\n",
    "        # vq: (Encoding) Assign words from the dictionary to each descripteur\n",
    "        words, distance = vq(des_list[i],dic)\n",
    "        \n",
    "        ### 3. Step: Pooling - calculate a histogram for each image\n",
    "        # Pooling refers to the process of representing an image as a \"bag of words\". \n",
    "        # The word bag here is meant to convey that once you have encoded each descripteur with a word  (a number between 1 and K), \n",
    "        # you build a new representation (a bag) that discards the spatial relationship between the words that \n",
    "        # constitute your image.\n",
    "\n",
    "        # This representation is often a histogram or a collection of spatially adjacent histograms of the desribteurs \n",
    "        # (i.e. histograms of values 1 to K) that together form your image. \"Pooling\" is thus the process of \n",
    "        # building a histogram of words (i.e. pooling ~ \"sampling\" words from the image to build a probability \n",
    "        # mass function of words)\n",
    "\n",
    "        # To clarify, the purpose of pooling is two fold:\n",
    "        #           By building a feature vector that is a histogram of words (as opposed to putting the full \"sentence of words\" \n",
    "        #           in the feature vector), your descriptor will be invariant to changes in \"the ordering of words\". \n",
    "        #           In computer vision this translates into invariance with respect to rotations and distortions of the image \n",
    "        #           and object, which is a desirable thing to have.\n",
    "\n",
    "        #           If the dictionary is small compared to the length of the sentence, a histogram of words has less dimensions \n",
    "        #           than the original vector. Less dimensions makes learning (training) much easier.\n",
    "        \n",
    "        \n",
    "        # Count the accuarance of each word (w) in image (i) to build histogram\n",
    "        for w in words:\n",
    "            npSurfHist[i][w] += 1\n",
    "        \n",
    "        #### 4. Step: Normalization of features vector (Can be changed to distribution like ColorHisto)\n",
    "        # Frequency divided by amount of words (k)\n",
    "        summe = sum(npSurfHist[i])\n",
    "        for x in range(0,k):\n",
    "            #npSurfHist[i][x] = npSurfHist[i][x]/k\n",
    "            npSurfHist[i][x] = npSurfHist[i][x]/summe\n",
    "        \n",
    "        #stdSlr = StandardScaler().fit(npSurfHist)\n",
    "        #npSurfHist = stdSlr.transform(npSurfHist)\n",
    "    \n",
    "    return npSurfHist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT Experimental - use SURF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 128)\n"
     ]
    }
   ],
   "source": [
    "########### Feature SIFT (Scale-invariant feature transform cf. http://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html#gsc.tab=0)\n",
    "# Api cf. http://docs.opencv.org/2.4/modules/nonfree/doc/feature_detection.html\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../../03-jeux-de-donnees/101_ObjectCategories/airplanes/image_0306.jpg')\n",
    "gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.SIFT(nfeatures=100)\n",
    "#sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Detector which detects the Keypoints in the Image\n",
    "#kp = sift.detect(gray,None)\n",
    "\n",
    "# Just a visualization of the Keypoints in the Image\n",
    "#img=cv2.drawKeypoints(gray,kp)\n",
    "#cv2.imwrite('D:\\Sift-test\\sift_keypoints.jpg',img)\n",
    "\n",
    "# Another visualization with FLAG: draw a circle with size of keypoint and it will even show its orientation\n",
    "#img=cv2.drawKeypoints(gray,kp,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "#cv2.imwrite('D:\\Sift-test\\sift_keypoints.jpg',img)\n",
    "\n",
    "# Method to compute the descripteurs after one has already detected the keypoints\n",
    "#kp,des = sift.compute(gray,kp)\n",
    "\n",
    "#sift = cv2.xfeatures2d.SIFT_create()\n",
    "#sift = cv2.SIFT()\n",
    "\n",
    "# Method to detect keypoints (kp) and calculate the descripteurs (des) with one function call\n",
    "kp, des = sift.detectAndCompute(gray,None)\n",
    "\n",
    "print (des.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to export calculated Data to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Export Features to csv\n",
    "def exportToCSV(pandasSorDF, filename):\n",
    "    #filename = datetime.datetime.now().strftime(\"%Y_%m_%d\") + \"-Feature\"\n",
    "    path = os.getcwdu() + \"\\\\\" + filename\n",
    "    \n",
    "    if os.path.isfile(path + \".csv\"):\n",
    "        for i in range(1,20):\n",
    "            testFileName = filename  + \"-\" + str(i) + \".csv\"\n",
    "            if os.path.isfile(os.getcwdu() + \"\\\\\" +  testFileName)!=True:\n",
    "                pandasSorDF.to_csv(testFileName)\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        pandasSorDF.to_csv(filename + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exportNumpyToCSV(numpyArray, filename):\n",
    "    #filename = datetime.datetime.now().strftime(\"%Y_%m_%d\") + \"-Feature\"\n",
    "    path = os.getcwdu() + \"\\\\\" + filename\n",
    "    \n",
    "    if os.path.isfile(path + \".csv\"):\n",
    "        for i in range(1,20):\n",
    "            testFileName = filename  + \"-\" + str(i) + \".csv\"\n",
    "            if os.path.isfile(os.getcwdu() + \"\\\\\" +  testFileName)!=True:\n",
    "                np.savetxt(testFileName, numpyArray, delimiter=\",\")\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        np.savetxt(filename + \".csv\", numpyArray, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Programm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os           # for iteration throug directories\n",
    "import pandas as pd # for Series and DataFrames\n",
    "import cv2          # for OpenCV \n",
    "import datetime     # for TimeStamp in CSVFile\n",
    "from scipy.cluster.vq import * # for Clustering http://docs.scipy.org/doc/scipy/reference/cluster.vq.html\n",
    "import numpy as np  # for arrays\n",
    "import time       # for time calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9145,)\n",
      "Time to extract all images: 28.9075498581\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Determine the Database to extract features\n",
    "path ='../../03-jeux-de-donnees/101_ObjectCategories'\n",
    "\n",
    "# get dictionary to link classLabels Text to Integers\n",
    "sClassLabels = getClassLabels(path)\n",
    "\n",
    "# Get all path from all images inclusive classLabel as Integer\n",
    "dfImages = imgCrawl(path, sClassLabels)\n",
    "\n",
    "print dfImages.classLabel.shape\n",
    "\n",
    "fileNameClassLabels = datetime.datetime.now().strftime(\"%Y_%m_%d\") + \"-Caltech-ClassLabels\"\n",
    "exportNumpyToCSV(dfImages.classLabel, fileNameClassLabels)\n",
    "\n",
    "fileNameClassLabels = datetime.datetime.now().strftime(\"%Y_%m_%d\") + \"-Caltech-ClassLabels-Description\"\n",
    "#exportToCSV(sClassLabels, fileNameClassLabels)\n",
    "\n",
    "end = time.time()\n",
    "print \"Time to extract all images: \" + str(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9145, 48)\n",
      "Time to calculate ColorHistogram: 41.924052\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Calculate Color Histogramm wit 16 bins for each color -> feature length = 3 x 16 = 48\n",
    "npColorHistogram = calcColorHisto(dfImages, 16)\n",
    "\n",
    "print npColorHistogram.shape\n",
    "\n",
    "fileNameColorHis = datetime.datetime.now().strftime(\"%Y_%m_%d\") + \"-Caltech-Feature-ColorHistogram\"\n",
    "#exportNumpyToCSV(npColorHistogram, fileNameColorHis)\n",
    "\n",
    "end = time.time()\n",
    "print \"Time to calculate ColorHistogram: \" + str(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-43b9b016b2b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Calculate Surf Histogramm with K=100 Cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnpSurfHistogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalcSurfHisto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mnpSurfHistogram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-c8d37ffc0446>\u001b[0m in \u001b[0;36mcalcSurfHisto\u001b[1;34m(dfImages_, k_)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdes_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mdescriptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescriptors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#### Bag of Words Approach\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/doob/anaconda2/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Calculate Surf Histogramm with K=100 Cluster\n",
    "npSurfHistogram = calcSurfHisto(dfImages, 100)\n",
    "\n",
    "print npSurfHistogram.shape\n",
    "\n",
    "fileNameSurfHis = datetime.datetime.now().strftime(\"%Y_%m_%d\") + \"-Caltech-Feature-SurfHistogram\"\n",
    "#exportNumpyToCSV(npSurfHistogram, fileNameSurfHis)\n",
    "\n",
    "end = time.time()\n",
    "print \"Time to calculate SurfHistogram: \" + str(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
