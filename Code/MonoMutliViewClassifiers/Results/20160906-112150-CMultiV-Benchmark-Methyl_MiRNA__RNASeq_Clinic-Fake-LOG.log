2016-09-06 11:21:50,451 DEBUG: Start:	 Creating 2 temporary datasets for multiprocessing
2016-09-06 11:21:50,452 WARNING:  WARNING : /!\ This may use a lot of HDD storage space : 0.000124 Gbytes /!\ 
2016-09-06 11:21:55,466 DEBUG: Start:	 Creating datasets for multiprocessing
2016-09-06 11:21:55,470 INFO: Start:	 Finding all available mono- & multiview algorithms
2016-09-06 11:21:55,524 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:55,525 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-06 11:21:55,525 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:55,525 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:55,525 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-06 11:21:55,525 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:55,526 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:55,526 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:55,526 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:55,526 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:55,526 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:55,526 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:55,526 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:55,526 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:55,561 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:55,561 DEBUG: Start:	 Training
2016-09-06 11:21:55,563 DEBUG: Info:	 Time for Training: 0.0388250350952[s]
2016-09-06 11:21:55,563 DEBUG: Done:	 Training
2016-09-06 11:21:55,563 DEBUG: Start:	 Predicting
2016-09-06 11:21:55,565 DEBUG: Done:	 Predicting
2016-09-06 11:21:55,565 DEBUG: Start:	 Getting Results
2016-09-06 11:21:55,575 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:55,575 DEBUG: Start:	 Training
2016-09-06 11:21:55,579 DEBUG: Info:	 Time for Training: 0.0551791191101[s]
2016-09-06 11:21:55,579 DEBUG: Done:	 Training
2016-09-06 11:21:55,579 DEBUG: Start:	 Predicting
2016-09-06 11:21:55,582 DEBUG: Done:	 Predicting
2016-09-06 11:21:55,582 DEBUG: Start:	 Getting Results
2016-09-06 11:21:55,604 DEBUG: Done:	 Getting Results
2016-09-06 11:21:55,605 INFO: Classification on Fake database for View0 with DecisionTree

accuracy_score on train : 1.0
accuracy_score on test : 0.433333333333

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.433333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.413793103448
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.413793103448
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.566666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.433333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.131912640639
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.391304347826
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.439024390244
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.433797909408
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.566666666667


 Classification took 0:00:00
2016-09-06 11:21:55,605 INFO: Done:	 Result Analysis
2016-09-06 11:21:55,619 DEBUG: Done:	 Getting Results
2016-09-06 11:21:55,619 INFO: Classification on Fake database for View0 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.5

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 7, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.494382022472
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.494382022472
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.5
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.00596274193664
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.458333333333
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.536585365854
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.502986560478
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.5


 Classification took 0:00:00
2016-09-06 11:21:55,619 INFO: Done:	 Result Analysis
2016-09-06 11:21:55,774 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:55,774 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:55,774 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-06 11:21:55,774 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-06 11:21:55,774 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:55,774 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:55,774 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:55,774 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:55,775 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:55,775 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:55,775 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:55,775 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:55,775 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:55,775 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:55,806 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:55,806 DEBUG: Start:	 Training
2016-09-06 11:21:55,807 DEBUG: Info:	 Time for Training: 0.0337290763855[s]
2016-09-06 11:21:55,807 DEBUG: Done:	 Training
2016-09-06 11:21:55,807 DEBUG: Start:	 Predicting
2016-09-06 11:21:55,814 DEBUG: Done:	 Predicting
2016-09-06 11:21:55,814 DEBUG: Start:	 Getting Results
2016-09-06 11:21:55,852 DEBUG: Done:	 Getting Results
2016-09-06 11:21:55,852 INFO: Classification on Fake database for View0 with KNN

accuracy_score on train : 0.557142857143
accuracy_score on test : 0.422222222222

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 47
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.557142857143
		- Score on test : 0.422222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.550724637681
		- Score on test : 0.315789473684
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.550724637681
		- Score on test : 0.315789473684
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.442857142857
		- Score on test : 0.577777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.557142857143
		- Score on test : 0.422222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.11473701202
		- Score on test : -0.180519041032
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.564356435644
		- Score on test : 0.342857142857
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.537735849057
		- Score on test : 0.292682926829
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.55732946299
		- Score on test : 0.411647585864
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.442857142857
		- Score on test : 0.577777777778


 Classification took 0:00:00
2016-09-06 11:21:55,852 INFO: Done:	 Result Analysis
2016-09-06 11:21:55,945 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:55,946 DEBUG: Start:	 Training
2016-09-06 11:21:55,964 DEBUG: Info:	 Time for Training: 0.190694093704[s]
2016-09-06 11:21:55,964 DEBUG: Done:	 Training
2016-09-06 11:21:55,964 DEBUG: Start:	 Predicting
2016-09-06 11:21:55,968 DEBUG: Done:	 Predicting
2016-09-06 11:21:55,968 DEBUG: Start:	 Getting Results
2016-09-06 11:21:55,996 DEBUG: Done:	 Getting Results
2016-09-06 11:21:55,996 INFO: Classification on Fake database for View0 with RandomForest

accuracy_score on train : 0.957142857143
accuracy_score on test : 0.4

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 7, max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.957142857143
		- Score on test : 0.4
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.956937799043
		- Score on test : 0.325
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.956937799043
		- Score on test : 0.325
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0428571428571
		- Score on test : 0.6
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.957142857143
		- Score on test : 0.4
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.914674537841
		- Score on test : -0.214609988978
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.970873786408
		- Score on test : 0.333333333333
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.943396226415
		- Score on test : 0.317073170732
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.957275036284
		- Score on test : 0.393230462917
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0428571428571
		- Score on test : 0.6


 Classification took 0:00:00
2016-09-06 11:21:55,997 INFO: Done:	 Result Analysis
2016-09-06 11:21:56,124 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,125 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-06 11:21:56,125 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,125 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,125 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-06 11:21:56,125 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,125 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:56,125 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:56,126 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:56,126 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,126 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:56,126 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,126 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,126 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,173 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:56,173 DEBUG: Start:	 Training
2016-09-06 11:21:56,174 DEBUG: Info:	 Time for Training: 0.0500919818878[s]
2016-09-06 11:21:56,174 DEBUG: Done:	 Training
2016-09-06 11:21:56,174 DEBUG: Start:	 Predicting
2016-09-06 11:21:56,181 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:56,181 DEBUG: Start:	 Training
2016-09-06 11:21:56,185 DEBUG: Done:	 Predicting
2016-09-06 11:21:56,185 DEBUG: Start:	 Getting Results
2016-09-06 11:21:56,202 DEBUG: Info:	 Time for Training: 0.0777740478516[s]
2016-09-06 11:21:56,202 DEBUG: Done:	 Training
2016-09-06 11:21:56,202 DEBUG: Start:	 Predicting
2016-09-06 11:21:56,206 DEBUG: Done:	 Predicting
2016-09-06 11:21:56,206 DEBUG: Start:	 Getting Results
2016-09-06 11:21:56,213 DEBUG: Done:	 Getting Results
2016-09-06 11:21:56,213 INFO: Classification on Fake database for View0 with SGD

accuracy_score on train : 0.604761904762
accuracy_score on test : 0.433333333333

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : modified_huber, penalty : l2
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.604761904762
		- Score on test : 0.433333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.62443438914
		- Score on test : 0.43956043956
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.62443438914
		- Score on test : 0.43956043956
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.395238095238
		- Score on test : 0.566666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.604761904762
		- Score on test : 0.433333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.209578877963
		- Score on test : -0.124719695673
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.6
		- Score on test : 0.4
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.650943396226
		- Score on test : 0.487804878049
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.604317851959
		- Score on test : 0.437779990045
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.395238095238
		- Score on test : 0.566666666667


 Classification took 0:00:00
2016-09-06 11:21:56,214 INFO: Done:	 Result Analysis
2016-09-06 11:21:56,250 DEBUG: Done:	 Getting Results
2016-09-06 11:21:56,251 INFO: Classification on Fake database for View0 with SVMLinear

accuracy_score on train : 0.495238095238
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.495238095238
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.504672897196
		- Score on test : 0.404761904762
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.504672897196
		- Score on test : 0.404761904762
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.504761904762
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.495238095238
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : -0.00980036362201
		- Score on test : -0.115633266975
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.395348837209
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.509433962264
		- Score on test : 0.414634146341
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.495101596517
		- Score on test : 0.442010950722
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.504761904762
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-06 11:21:56,251 INFO: Done:	 Result Analysis
2016-09-06 11:21:56,375 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,375 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,376 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMRBF
2016-09-06 11:21:56,376 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMPoly
2016-09-06 11:21:56,376 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,376 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,377 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:56,377 DEBUG: Info:	 Shape X_train:(210, 13), Length of y_train:210
2016-09-06 11:21:56,377 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:56,377 DEBUG: Info:	 Shape X_test:(90, 13), Length of y_test:90
2016-09-06 11:21:56,377 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,377 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,377 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,377 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,447 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:56,448 DEBUG: Start:	 Training
2016-09-06 11:21:56,458 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:56,458 DEBUG: Start:	 Training
2016-09-06 11:21:56,471 DEBUG: Info:	 Time for Training: 0.0959920883179[s]
2016-09-06 11:21:56,471 DEBUG: Done:	 Training
2016-09-06 11:21:56,471 DEBUG: Start:	 Predicting
2016-09-06 11:21:56,479 DEBUG: Done:	 Predicting
2016-09-06 11:21:56,479 DEBUG: Start:	 Getting Results
2016-09-06 11:21:56,482 DEBUG: Info:	 Time for Training: 0.107470989227[s]
2016-09-06 11:21:56,482 DEBUG: Done:	 Training
2016-09-06 11:21:56,483 DEBUG: Start:	 Predicting
2016-09-06 11:21:56,488 DEBUG: Done:	 Predicting
2016-09-06 11:21:56,488 DEBUG: Start:	 Getting Results
2016-09-06 11:21:56,517 DEBUG: Done:	 Getting Results
2016-09-06 11:21:56,517 INFO: Classification on Fake database for View0 with SVMRBF

accuracy_score on train : 1.0
accuracy_score on test : 0.411111111111

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.411111111111
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.345679012346
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.345679012346
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.588888888889
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.411111111111
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.189573937423
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.35
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.341463414634
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.405425584868
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.588888888889


 Classification took 0:00:00
2016-09-06 11:21:56,518 INFO: Done:	 Result Analysis
2016-09-06 11:21:56,536 DEBUG: Done:	 Getting Results
2016-09-06 11:21:56,537 INFO: Classification on Fake database for View0 with SVMPoly

accuracy_score on train : 1.0
accuracy_score on test : 0.5

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 13)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.150943396226
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.150943396226
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.5
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.096260040145
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.333333333333
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.0975609756098
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.467147834744
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.5


 Classification took 0:00:00
2016-09-06 11:21:56,537 INFO: Done:	 Result Analysis
2016-09-06 11:21:56,625 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,625 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,625 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-06 11:21:56,625 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-06 11:21:56,626 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,626 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,626 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:56,626 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:56,626 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:56,626 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:56,626 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,626 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,626 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,626 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,664 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:56,664 DEBUG: Start:	 Training
2016-09-06 11:21:56,666 DEBUG: Info:	 Time for Training: 0.0415709018707[s]
2016-09-06 11:21:56,666 DEBUG: Done:	 Training
2016-09-06 11:21:56,666 DEBUG: Start:	 Predicting
2016-09-06 11:21:56,669 DEBUG: Done:	 Predicting
2016-09-06 11:21:56,669 DEBUG: Start:	 Getting Results
2016-09-06 11:21:56,678 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:56,678 DEBUG: Start:	 Training
2016-09-06 11:21:56,684 DEBUG: Info:	 Time for Training: 0.059385061264[s]
2016-09-06 11:21:56,684 DEBUG: Done:	 Training
2016-09-06 11:21:56,684 DEBUG: Start:	 Predicting
2016-09-06 11:21:56,688 DEBUG: Done:	 Predicting
2016-09-06 11:21:56,688 DEBUG: Start:	 Getting Results
2016-09-06 11:21:56,718 DEBUG: Done:	 Getting Results
2016-09-06 11:21:56,718 INFO: Classification on Fake database for View1 with DecisionTree

accuracy_score on train : 1.0
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.418604651163
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.418604651163
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.111552687063
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.4
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.439024390244
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.44400199104
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-06 11:21:56,719 INFO: Done:	 Result Analysis
2016-09-06 11:21:56,731 DEBUG: Done:	 Getting Results
2016-09-06 11:21:56,731 INFO: Classification on Fake database for View1 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 7, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.404761904762
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.404761904762
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.115633266975
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.395348837209
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.414634146341
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.442010950722
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-06 11:21:56,731 INFO: Done:	 Result Analysis
2016-09-06 11:21:56,878 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,878 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-06 11:21:56,878 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,879 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:56,879 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:56,879 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,879 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,879 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:56,880 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-06 11:21:56,880 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:56,880 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:56,881 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:56,881 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:56,881 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:56,909 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:56,909 DEBUG: Start:	 Training
2016-09-06 11:21:56,910 DEBUG: Info:	 Time for Training: 0.032821893692[s]
2016-09-06 11:21:56,910 DEBUG: Done:	 Training
2016-09-06 11:21:56,910 DEBUG: Start:	 Predicting
2016-09-06 11:21:56,917 DEBUG: Done:	 Predicting
2016-09-06 11:21:56,918 DEBUG: Start:	 Getting Results
2016-09-06 11:21:56,972 DEBUG: Done:	 Getting Results
2016-09-06 11:21:56,973 INFO: Classification on Fake database for View1 with KNN

accuracy_score on train : 0.547619047619
accuracy_score on test : 0.455555555556

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 47
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.547619047619
		- Score on test : 0.455555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.633204633205
		- Score on test : 0.558558558559
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.633204633205
		- Score on test : 0.558558558559
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.452380952381
		- Score on test : 0.544444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.547619047619
		- Score on test : 0.455555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.102191547553
		- Score on test : -0.0477019354931
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.535947712418
		- Score on test : 0.442857142857
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.77358490566
		- Score on test : 0.756097560976
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.545446298984
		- Score on test : 0.480089596814
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.452380952381
		- Score on test : 0.544444444444


 Classification took 0:00:00
2016-09-06 11:21:56,973 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,059 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:57,059 DEBUG: Start:	 Training
2016-09-06 11:21:57,082 DEBUG: Info:	 Time for Training: 0.203009128571[s]
2016-09-06 11:21:57,082 DEBUG: Done:	 Training
2016-09-06 11:21:57,082 DEBUG: Start:	 Predicting
2016-09-06 11:21:57,088 DEBUG: Done:	 Predicting
2016-09-06 11:21:57,088 DEBUG: Start:	 Getting Results
2016-09-06 11:21:57,136 DEBUG: Done:	 Getting Results
2016-09-06 11:21:57,136 INFO: Classification on Fake database for View1 with RandomForest

accuracy_score on train : 0.97619047619
accuracy_score on test : 0.522222222222

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 7, max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.97619047619
		- Score on test : 0.522222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.976525821596
		- Score on test : 0.516853932584
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.976525821596
		- Score on test : 0.516853932584
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0238095238095
		- Score on test : 0.477777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.97619047619
		- Score on test : 0.522222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.952415522541
		- Score on test : 0.0506833064614
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.971962616822
		- Score on test : 0.479166666667
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.981132075472
		- Score on test : 0.560975609756
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.976142960813
		- Score on test : 0.525385764062
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0238095238095
		- Score on test : 0.477777777778


 Classification took 0:00:00
2016-09-06 11:21:57,136 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,224 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,224 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-06 11:21:57,224 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,225 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,225 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-06 11:21:57,225 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,225 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:57,225 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:57,225 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:57,225 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,225 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:57,226 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,226 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:57,226 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:57,271 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:57,271 DEBUG: Start:	 Training
2016-09-06 11:21:57,272 DEBUG: Info:	 Time for Training: 0.0480880737305[s]
2016-09-06 11:21:57,272 DEBUG: Done:	 Training
2016-09-06 11:21:57,272 DEBUG: Start:	 Predicting
2016-09-06 11:21:57,281 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:57,281 DEBUG: Start:	 Training
2016-09-06 11:21:57,288 DEBUG: Done:	 Predicting
2016-09-06 11:21:57,289 DEBUG: Start:	 Getting Results
2016-09-06 11:21:57,310 DEBUG: Info:	 Time for Training: 0.0865099430084[s]
2016-09-06 11:21:57,311 DEBUG: Done:	 Training
2016-09-06 11:21:57,311 DEBUG: Start:	 Predicting
2016-09-06 11:21:57,315 DEBUG: Done:	 Predicting
2016-09-06 11:21:57,315 DEBUG: Start:	 Getting Results
2016-09-06 11:21:57,316 DEBUG: Done:	 Getting Results
2016-09-06 11:21:57,316 INFO: Classification on Fake database for View1 with SGD

accuracy_score on train : 0.566666666667
accuracy_score on test : 0.522222222222

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : modified_huber, penalty : l2
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.566666666667
		- Score on test : 0.522222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.564593301435
		- Score on test : 0.516853932584
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.564593301435
		- Score on test : 0.516853932584
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.433333333333
		- Score on test : 0.477777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.566666666667
		- Score on test : 0.522222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.133545022783
		- Score on test : 0.0506833064614
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.572815533981
		- Score on test : 0.479166666667
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.556603773585
		- Score on test : 0.560975609756
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.566763425254
		- Score on test : 0.525385764062
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.433333333333
		- Score on test : 0.477777777778


 Classification took 0:00:00
2016-09-06 11:21:57,316 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,383 DEBUG: Done:	 Getting Results
2016-09-06 11:21:57,383 INFO: Classification on Fake database for View1 with SVMLinear

accuracy_score on train : 0.528571428571
accuracy_score on test : 0.533333333333

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.528571428571
		- Score on test : 0.533333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.535211267606
		- Score on test : 0.475
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.535211267606
		- Score on test : 0.475
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.471428571429
		- Score on test : 0.466666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.528571428571
		- Score on test : 0.533333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0569743711331
		- Score on test : 0.0555284586866
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.532710280374
		- Score on test : 0.487179487179
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.537735849057
		- Score on test : 0.463414634146
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.528483309144
		- Score on test : 0.52762568442
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.471428571429
		- Score on test : 0.466666666667


 Classification took 0:00:00
2016-09-06 11:21:57,383 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,473 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,473 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,474 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMRBF
2016-09-06 11:21:57,474 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMPoly
2016-09-06 11:21:57,474 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,474 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,475 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:57,475 DEBUG: Info:	 Shape X_train:(210, 18), Length of y_train:210
2016-09-06 11:21:57,475 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:57,475 DEBUG: Info:	 Shape X_test:(90, 18), Length of y_test:90
2016-09-06 11:21:57,475 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,475 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,475 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:57,475 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:57,549 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:57,549 DEBUG: Start:	 Training
2016-09-06 11:21:57,561 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:57,561 DEBUG: Start:	 Training
2016-09-06 11:21:57,573 DEBUG: Info:	 Time for Training: 0.100679159164[s]
2016-09-06 11:21:57,574 DEBUG: Done:	 Training
2016-09-06 11:21:57,574 DEBUG: Start:	 Predicting
2016-09-06 11:21:57,582 DEBUG: Done:	 Predicting
2016-09-06 11:21:57,582 DEBUG: Start:	 Getting Results
2016-09-06 11:21:57,590 DEBUG: Info:	 Time for Training: 0.117436170578[s]
2016-09-06 11:21:57,590 DEBUG: Done:	 Training
2016-09-06 11:21:57,590 DEBUG: Start:	 Predicting
2016-09-06 11:21:57,596 DEBUG: Done:	 Predicting
2016-09-06 11:21:57,597 DEBUG: Start:	 Getting Results
2016-09-06 11:21:57,627 DEBUG: Done:	 Getting Results
2016-09-06 11:21:57,627 INFO: Classification on Fake database for View1 with SVMRBF

accuracy_score on train : 1.0
accuracy_score on test : 0.533333333333

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.511627906977
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.511627906977
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.466666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.066931612238
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.488888888889
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.536585365854
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533598805376
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.466666666667


 Classification took 0:00:00
2016-09-06 11:21:57,627 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,645 DEBUG: Done:	 Getting Results
2016-09-06 11:21:57,645 INFO: Classification on Fake database for View1 with SVMPoly

accuracy_score on train : 1.0
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 18)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.561403508772
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.561403508772
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.0715653145323
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.438356164384
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.780487804878
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.4718765555
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-06 11:21:57,645 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,719 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,719 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-06 11:21:57,719 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,719 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,719 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-06 11:21:57,719 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,720 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:57,720 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:57,720 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:57,720 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:57,720 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,720 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,720 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:57,720 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:57,753 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:57,753 DEBUG: Start:	 Training
2016-09-06 11:21:57,755 DEBUG: Info:	 Time for Training: 0.0360541343689[s]
2016-09-06 11:21:57,755 DEBUG: Done:	 Training
2016-09-06 11:21:57,755 DEBUG: Start:	 Predicting
2016-09-06 11:21:57,757 DEBUG: Done:	 Predicting
2016-09-06 11:21:57,757 DEBUG: Start:	 Getting Results
2016-09-06 11:21:57,767 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:57,767 DEBUG: Start:	 Training
2016-09-06 11:21:57,771 DEBUG: Info:	 Time for Training: 0.052619934082[s]
2016-09-06 11:21:57,771 DEBUG: Done:	 Training
2016-09-06 11:21:57,771 DEBUG: Start:	 Predicting
2016-09-06 11:21:57,774 DEBUG: Done:	 Predicting
2016-09-06 11:21:57,774 DEBUG: Start:	 Getting Results
2016-09-06 11:21:57,802 DEBUG: Done:	 Getting Results
2016-09-06 11:21:57,802 INFO: Classification on Fake database for View2 with DecisionTree

accuracy_score on train : 0.995238095238
accuracy_score on test : 0.533333333333

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.533333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.995305164319
		- Score on test : 0.511627906977
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.995305164319
		- Score on test : 0.511627906977
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.466666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.533333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.990519401324
		- Score on test : 0.066931612238
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.990654205607
		- Score on test : 0.488888888889
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.536585365854
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.995192307692
		- Score on test : 0.533598805376
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.466666666667


 Classification took 0:00:00
2016-09-06 11:21:57,803 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,820 DEBUG: Done:	 Getting Results
2016-09-06 11:21:57,820 INFO: Classification on Fake database for View2 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.555555555556

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 7, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.555555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.512195121951
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.512195121951
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.444444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.555555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.104031856645
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.512195121951
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.512195121951
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.552015928323
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.444444444444


 Classification took 0:00:00
2016-09-06 11:21:57,820 INFO: Done:	 Result Analysis
2016-09-06 11:21:57,970 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,970 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:57,970 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-06 11:21:57,970 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-06 11:21:57,970 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,970 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:57,971 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:57,971 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:57,971 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:57,971 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:57,971 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,971 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:57,971 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:57,971 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:58,004 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,004 DEBUG: Start:	 Training
2016-09-06 11:21:58,005 DEBUG: Info:	 Time for Training: 0.0356760025024[s]
2016-09-06 11:21:58,005 DEBUG: Done:	 Training
2016-09-06 11:21:58,005 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,013 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,013 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,054 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,054 INFO: Classification on Fake database for View2 with KNN

accuracy_score on train : 0.552380952381
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 47
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.552380952381
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.477777777778
		- Score on test : 0.342105263158
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.477777777778
		- Score on test : 0.342105263158
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.447619047619
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.552380952381
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.112597765671
		- Score on test : -0.134753650348
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.581081081081
		- Score on test : 0.371428571429
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.405660377358
		- Score on test : 0.317073170732
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.553791727141
		- Score on test : 0.434046789447
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.447619047619
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-06 11:21:58,054 INFO: Done:	 Result Analysis
2016-09-06 11:21:58,134 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,134 DEBUG: Start:	 Training
2016-09-06 11:21:58,151 DEBUG: Info:	 Time for Training: 0.181129932404[s]
2016-09-06 11:21:58,151 DEBUG: Done:	 Training
2016-09-06 11:21:58,151 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,154 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,155 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,182 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,182 INFO: Classification on Fake database for View2 with RandomForest

accuracy_score on train : 0.966666666667
accuracy_score on test : 0.588888888889

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 7, max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.966666666667
		- Score on test : 0.588888888889
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.96682464455
		- Score on test : 0.53164556962
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.96682464455
		- Score on test : 0.53164556962
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0333333333333
		- Score on test : 0.411111111111
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.966666666667
		- Score on test : 0.588888888889
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.933375664255
		- Score on test : 0.166630556676
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.971428571429
		- Score on test : 0.552631578947
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.962264150943
		- Score on test : 0.512195121951
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.966708998549
		- Score on test : 0.582628173221
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0333333333333
		- Score on test : 0.411111111111


 Classification took 0:00:00
2016-09-06 11:21:58,182 INFO: Done:	 Result Analysis
2016-09-06 11:21:58,321 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:58,321 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-06 11:21:58,321 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:58,321 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:58,321 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-06 11:21:58,322 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:58,322 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:58,322 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:58,322 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:58,322 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:58,323 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:58,323 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:58,323 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:58,323 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:58,392 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,392 DEBUG: Start:	 Training
2016-09-06 11:21:58,393 DEBUG: Info:	 Time for Training: 0.0729160308838[s]
2016-09-06 11:21:58,393 DEBUG: Done:	 Training
2016-09-06 11:21:58,393 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,396 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,396 DEBUG: Start:	 Training
2016-09-06 11:21:58,406 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,406 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,415 DEBUG: Info:	 Time for Training: 0.095123052597[s]
2016-09-06 11:21:58,416 DEBUG: Done:	 Training
2016-09-06 11:21:58,416 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,419 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,419 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,431 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,431 INFO: Classification on Fake database for View2 with SGD

accuracy_score on train : 0.561904761905
accuracy_score on test : 0.5

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : modified_huber, penalty : l2
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.561904761905
		- Score on test : 0.5
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.544554455446
		- Score on test : 0.470588235294
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.544554455446
		- Score on test : 0.470588235294
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.438095238095
		- Score on test : 0.5
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.561904761905
		- Score on test : 0.5
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.125091870983
		- Score on test : -0.00198364873142
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.572916666667
		- Score on test : 0.454545454545
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.518867924528
		- Score on test : 0.487804878049
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.562318577649
		- Score on test : 0.499004479841
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.438095238095
		- Score on test : 0.5


 Classification took 0:00:00
2016-09-06 11:21:58,431 INFO: Done:	 Result Analysis
2016-09-06 11:21:58,448 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,448 INFO: Classification on Fake database for View2 with SVMLinear

accuracy_score on train : 0.495238095238
accuracy_score on test : 0.633333333333

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.495238095238
		- Score on test : 0.633333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.51376146789
		- Score on test : 0.637362637363
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.51376146789
		- Score on test : 0.637362637363
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.504761904762
		- Score on test : 0.366666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.495238095238
		- Score on test : 0.633333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : -0.0101818424163
		- Score on test : 0.279372118308
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.58
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.528301886792
		- Score on test : 0.707317073171
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.494920174165
		- Score on test : 0.6393728223
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.504761904762
		- Score on test : 0.366666666667


 Classification took 0:00:00
2016-09-06 11:21:58,449 INFO: Done:	 Result Analysis
2016-09-06 11:21:58,568 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:58,568 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:58,568 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMPoly
2016-09-06 11:21:58,568 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMRBF
2016-09-06 11:21:58,568 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:58,568 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:58,569 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:58,569 DEBUG: Info:	 Shape X_train:(210, 8), Length of y_train:210
2016-09-06 11:21:58,569 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:58,569 DEBUG: Info:	 Shape X_test:(90, 8), Length of y_test:90
2016-09-06 11:21:58,569 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:58,569 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:58,569 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:58,569 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:58,614 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,614 DEBUG: Start:	 Training
2016-09-06 11:21:58,619 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,619 DEBUG: Start:	 Training
2016-09-06 11:21:58,632 DEBUG: Info:	 Time for Training: 0.064493894577[s]
2016-09-06 11:21:58,632 DEBUG: Done:	 Training
2016-09-06 11:21:58,632 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,637 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,637 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,638 DEBUG: Info:	 Time for Training: 0.0703361034393[s]
2016-09-06 11:21:58,638 DEBUG: Done:	 Training
2016-09-06 11:21:58,638 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,641 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,641 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,668 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,668 INFO: Classification on Fake database for View2 with SVMRBF

accuracy_score on train : 1.0
accuracy_score on test : 0.466666666667

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.466666666667
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.466666666667
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.466666666667
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.533333333333
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.466666666667
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.0592334494774
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.428571428571
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.512195121951
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.470383275261
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.533333333333


 Classification took 0:00:00
2016-09-06 11:21:58,668 INFO: Done:	 Result Analysis
2016-09-06 11:21:58,680 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,680 INFO: Classification on Fake database for View2 with SVMPoly

accuracy_score on train : 0.995238095238
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 8)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.995260663507
		- Score on test : 0.479166666667
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.995260663507
		- Score on test : 0.479166666667
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.990521113087
		- Score on test : -0.0940733030728
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.418181818182
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.990566037736
		- Score on test : 0.560975609756
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.995283018868
		- Score on test : 0.453957192633
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-06 11:21:58,680 INFO: Done:	 Result Analysis
2016-09-06 11:21:58,822 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:58,822 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:58,822 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-06 11:21:58,822 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-06 11:21:58,822 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:58,822 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:58,823 DEBUG: Info:	 Shape X_train:(210, 6), Length of y_train:210
2016-09-06 11:21:58,823 DEBUG: Info:	 Shape X_train:(210, 6), Length of y_train:210
2016-09-06 11:21:58,823 DEBUG: Info:	 Shape X_test:(90, 6), Length of y_test:90
2016-09-06 11:21:58,823 DEBUG: Info:	 Shape X_test:(90, 6), Length of y_test:90
2016-09-06 11:21:58,823 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:58,823 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:58,823 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:58,823 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:58,874 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,874 DEBUG: Start:	 Training
2016-09-06 11:21:58,875 DEBUG: Info:	 Time for Training: 0.0545539855957[s]
2016-09-06 11:21:58,876 DEBUG: Done:	 Training
2016-09-06 11:21:58,876 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,880 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,880 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,899 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:58,899 DEBUG: Start:	 Training
2016-09-06 11:21:58,905 DEBUG: Info:	 Time for Training: 0.0836429595947[s]
2016-09-06 11:21:58,905 DEBUG: Done:	 Training
2016-09-06 11:21:58,905 DEBUG: Start:	 Predicting
2016-09-06 11:21:58,908 DEBUG: Done:	 Predicting
2016-09-06 11:21:58,908 DEBUG: Start:	 Getting Results
2016-09-06 11:21:58,930 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,930 INFO: Classification on Fake database for View3 with DecisionTree

accuracy_score on train : 1.0
accuracy_score on test : 0.466666666667

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 6)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.466666666667
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.428571428571
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.428571428571
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.533333333333
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.466666666667
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.0709680565554
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.418604651163
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.439024390244
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.464410154306
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.533333333333


 Classification took 0:00:00
2016-09-06 11:21:58,931 INFO: Done:	 Result Analysis
2016-09-06 11:21:58,947 DEBUG: Done:	 Getting Results
2016-09-06 11:21:58,947 INFO: Classification on Fake database for View3 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 6)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 7, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.404761904762
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.404761904762
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.115633266975
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.395348837209
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.414634146341
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.442010950722
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-06 11:21:58,947 INFO: Done:	 Result Analysis
2016-09-06 11:21:59,070 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:59,070 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:59,070 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-06 11:21:59,070 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-06 11:21:59,070 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:59,070 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:59,071 DEBUG: Info:	 Shape X_train:(210, 6), Length of y_train:210
2016-09-06 11:21:59,071 DEBUG: Info:	 Shape X_train:(210, 6), Length of y_train:210
2016-09-06 11:21:59,071 DEBUG: Info:	 Shape X_test:(90, 6), Length of y_test:90
2016-09-06 11:21:59,071 DEBUG: Info:	 Shape X_test:(90, 6), Length of y_test:90
2016-09-06 11:21:59,071 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:59,071 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:59,071 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:59,071 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:59,101 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:59,102 DEBUG: Start:	 Training
2016-09-06 11:21:59,102 DEBUG: Info:	 Time for Training: 0.0325701236725[s]
2016-09-06 11:21:59,102 DEBUG: Done:	 Training
2016-09-06 11:21:59,102 DEBUG: Start:	 Predicting
2016-09-06 11:21:59,108 DEBUG: Done:	 Predicting
2016-09-06 11:21:59,108 DEBUG: Start:	 Getting Results
2016-09-06 11:21:59,150 DEBUG: Done:	 Getting Results
2016-09-06 11:21:59,150 INFO: Classification on Fake database for View3 with KNN

accuracy_score on train : 0.519047619048
accuracy_score on test : 0.411111111111

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 6)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 47
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.519047619048
		- Score on test : 0.411111111111
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.562770562771
		- Score on test : 0.453608247423
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.562770562771
		- Score on test : 0.453608247423
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.480952380952
		- Score on test : 0.588888888889
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.519047619048
		- Score on test : 0.411111111111
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0369594857345
		- Score on test : -0.161571085301
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.52
		- Score on test : 0.392857142857
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.61320754717
		- Score on test : 0.536585365854
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.518142235123
		- Score on test : 0.421353907417
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.480952380952
		- Score on test : 0.588888888889


 Classification took 0:00:00
2016-09-06 11:21:59,150 INFO: Done:	 Result Analysis
2016-09-06 11:21:59,228 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:59,228 DEBUG: Start:	 Training
2016-09-06 11:21:59,246 DEBUG: Info:	 Time for Training: 0.176759958267[s]
2016-09-06 11:21:59,246 DEBUG: Done:	 Training
2016-09-06 11:21:59,246 DEBUG: Start:	 Predicting
2016-09-06 11:21:59,250 DEBUG: Done:	 Predicting
2016-09-06 11:21:59,250 DEBUG: Start:	 Getting Results
2016-09-06 11:21:59,282 DEBUG: Done:	 Getting Results
2016-09-06 11:21:59,282 INFO: Classification on Fake database for View3 with RandomForest

accuracy_score on train : 0.97619047619
accuracy_score on test : 0.488888888889

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 6)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 7, max_depth : 15
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.97619047619
		- Score on test : 0.488888888889
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.976303317536
		- Score on test : 0.410256410256
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.976303317536
		- Score on test : 0.410256410256
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0238095238095
		- Score on test : 0.511111111111
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.97619047619
		- Score on test : 0.488888888889
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.952424147199
		- Score on test : -0.0387937676182
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.980952380952
		- Score on test : 0.432432432432
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.971698113208
		- Score on test : 0.390243902439
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.976233671988
		- Score on test : 0.480836236934
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0238095238095
		- Score on test : 0.511111111111


 Classification took 0:00:00
2016-09-06 11:21:59,283 INFO: Done:	 Result Analysis
2016-09-06 11:21:59,419 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:59,419 DEBUG: ### Main Programm for Classification MonoView
2016-09-06 11:21:59,419 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-06 11:21:59,419 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-06 11:21:59,419 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:59,419 DEBUG: Start:	 Determine Train/Test split
2016-09-06 11:21:59,419 DEBUG: Info:	 Shape X_train:(210, 6), Length of y_train:210
2016-09-06 11:21:59,419 DEBUG: Info:	 Shape X_train:(210, 6), Length of y_train:210
2016-09-06 11:21:59,420 DEBUG: Info:	 Shape X_test:(90, 6), Length of y_test:90
2016-09-06 11:21:59,420 DEBUG: Info:	 Shape X_test:(90, 6), Length of y_test:90
2016-09-06 11:21:59,420 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:59,420 DEBUG: Done:	 Determine Train/Test split
2016-09-06 11:21:59,420 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:59,420 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-06 11:21:59,464 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:59,464 DEBUG: Start:	 Training
2016-09-06 11:21:59,465 DEBUG: Info:	 Time for Training: 0.0465881824493[s]
2016-09-06 11:21:59,465 DEBUG: Done:	 Training
2016-09-06 11:21:59,465 DEBUG: Start:	 Predicting
2016-09-06 11:21:59,468 DEBUG: Done:	 RandomSearch best settings
2016-09-06 11:21:59,468 DEBUG: Start:	 Training
2016-09-06 11:21:59,481 DEBUG: Done:	 Predicting
2016-09-06 11:21:59,481 DEBUG: Start:	 Getting Results
2016-09-06 11:21:59,487 DEBUG: Info:	 Time for Training: 0.0686330795288[s]
2016-09-06 11:21:59,487 DEBUG: Done:	 Training
2016-09-06 11:21:59,487 DEBUG: Start:	 Predicting
2016-09-06 11:21:59,490 DEBUG: Done:	 Predicting
2016-09-06 11:21:59,490 DEBUG: Start:	 Getting Results
2016-09-06 11:21:59,505 DEBUG: Done:	 Getting Results
2016-09-06 11:21:59,505 INFO: Classification on Fake database for View3 with SGD

accuracy_score on train : 0.542857142857
accuracy_score on test : 0.466666666667

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 6)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : modified_huber, penalty : l2
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.542857142857
		- Score on test : 0.466666666667
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.559633027523
		- Score on test : 0.478260869565
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.559633027523
		- Score on test : 0.478260869565
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.457142857143
		- Score on test : 0.533333333333
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.542857142857
		- Score on test : 0.466666666667
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0852729302366
		- Score on test : -0.0555284586866
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.544642857143
		- Score on test : 0.43137254902
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.575471698113
		- Score on test : 0.536585365854
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.542543541364
		- Score on test : 0.47237431558
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.457142857143
		- Score on test : 0.533333333333


 Classification took 0:00:00
2016-09-06 11:21:59,505 INFO: Done:	 Result Analysis
2016-09-06 11:21:59,523 DEBUG: Done:	 Getting Results
2016-09-06 11:21:59,523 INFO: Classification on Fake database for View3 with SVMLinear

accuracy_score on train : 0.557142857143
accuracy_score on test : 0.566666666667

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 6)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 2991
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.557142857143
		- Score on test : 0.566666666667
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.610878661088
		- Score on test : 0.571428571429
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.610878661088
		- Score on test : 0.571428571429
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.442857142857
		- Score on test : 0.433333333333
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.557142857143
		- Score on test : 0.566666666667
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.11594977827
		- Score on test : 0.144674846981
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.548872180451
		- Score on test : 0.52
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.688679245283
		- Score on test : 0.634146341463
	For ROS AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.55587808418
		- Score on test : 0.572175211548
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.442857142857
		- Score on test : 0.433333333333


 Classification took 0:00:00
2016-09-06 11:21:59,523 INFO: Done:	 Result Analysis
2016-09-06 11:21:59,818 INFO: ### Main Programm for Multiview Classification
2016-09-06 11:21:59,818 INFO: ### Classification - Database : Fake ; Views : Methyl, MiRNA_, RNASeq, Clinic ; Algorithm : Mumbo ; Cores : 1
2016-09-06 11:21:59,819 INFO: Info:	 Shape of View0 :(300, 13)
2016-09-06 11:21:59,819 INFO: Info:	 Shape of View1 :(300, 18)
2016-09-06 11:21:59,820 INFO: Info:	 Shape of View2 :(300, 8)
2016-09-06 11:21:59,820 INFO: Info:	 Shape of View3 :(300, 6)
2016-09-06 11:21:59,820 INFO: Done:	 Read Database Files
2016-09-06 11:21:59,820 INFO: Start:	 Determine validation split for ratio 0.7
2016-09-06 11:21:59,823 INFO: ### Main Programm for Multiview Classification
2016-09-06 11:21:59,823 INFO: ### Classification - Database : Fake ; Views : Methyl, MiRNA_, RNASeq, Clinic ; Algorithm : Fusion ; Cores : 1
2016-09-06 11:21:59,823 INFO: Info:	 Shape of View0 :(300, 13)
2016-09-06 11:21:59,824 INFO: Info:	 Shape of View1 :(300, 18)
2016-09-06 11:21:59,824 INFO: Done:	 Determine validation split
2016-09-06 11:21:59,824 INFO: Start:	 Determine 5 folds
2016-09-06 11:21:59,824 INFO: Info:	 Shape of View2 :(300, 8)
2016-09-06 11:21:59,825 INFO: Info:	 Shape of View3 :(300, 6)
2016-09-06 11:21:59,825 INFO: Done:	 Read Database Files
2016-09-06 11:21:59,825 INFO: Start:	 Determine validation split for ratio 0.7
2016-09-06 11:21:59,829 INFO: Done:	 Determine validation split
2016-09-06 11:21:59,829 INFO: Start:	 Determine 5 folds
2016-09-06 11:21:59,830 INFO: Info:	 Length of Learning Sets: 170
2016-09-06 11:21:59,831 INFO: Info:	 Length of Testing Sets: 41
2016-09-06 11:21:59,831 INFO: Info:	 Length of Validation Set: 89
2016-09-06 11:21:59,831 INFO: Done:	 Determine folds
2016-09-06 11:21:59,831 INFO: Start:	 Learning with Mumbo and 5 folds
2016-09-06 11:21:59,831 INFO: Start:	 Randomsearching best settings for monoview classifiers
2016-09-06 11:21:59,831 DEBUG: 	Start:	 Gridsearch for DecisionTree on View0
2016-09-06 11:21:59,834 INFO: Info:	 Length of Learning Sets: 170
2016-09-06 11:21:59,834 INFO: Info:	 Length of Testing Sets: 41
2016-09-06 11:21:59,834 INFO: Info:	 Length of Validation Set: 89
2016-09-06 11:21:59,834 INFO: Done:	 Determine folds
2016-09-06 11:21:59,834 INFO: Start:	 Learning with Fusion and 5 folds
2016-09-06 11:21:59,834 INFO: Start:	 Randomsearching best settings for monoview classifiers
2016-09-06 11:21:59,834 DEBUG: 	Start:	 Random search for Adaboost with 30 iterations
