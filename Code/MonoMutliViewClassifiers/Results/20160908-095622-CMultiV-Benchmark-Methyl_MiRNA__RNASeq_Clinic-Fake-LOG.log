2016-09-08 09:56:22,282 DEBUG: Start:	 Creating 2 temporary datasets for multiprocessing
2016-09-08 09:56:22,282 WARNING:  WARNING : /!\ This may use a lot of HDD storage space : 0.0001661875 Gbytes /!\ 
2016-09-08 09:56:27,294 DEBUG: Start:	 Creating datasets for multiprocessing
2016-09-08 09:56:27,298 INFO: Start:	 Finding all available mono- & multiview algorithms
2016-09-08 09:56:27,351 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:27,351 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-08 09:56:27,351 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:27,352 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:27,352 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-08 09:56:27,352 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:27,352 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:27,352 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:27,352 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:27,353 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:27,353 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:27,353 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:27,353 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:27,354 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:27,389 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:27,389 DEBUG: Start:	 Training
2016-09-08 09:56:27,391 DEBUG: Info:	 Time for Training: 0.0395510196686[s]
2016-09-08 09:56:27,391 DEBUG: Done:	 Training
2016-09-08 09:56:27,391 DEBUG: Start:	 Predicting
2016-09-08 09:56:27,393 DEBUG: Done:	 Predicting
2016-09-08 09:56:27,393 DEBUG: Start:	 Getting Results
2016-09-08 09:56:27,403 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:27,403 DEBUG: Start:	 Training
2016-09-08 09:56:27,407 DEBUG: Info:	 Time for Training: 0.0563409328461[s]
2016-09-08 09:56:27,407 DEBUG: Done:	 Training
2016-09-08 09:56:27,407 DEBUG: Start:	 Predicting
2016-09-08 09:56:27,410 DEBUG: Done:	 Predicting
2016-09-08 09:56:27,410 DEBUG: Start:	 Getting Results
2016-09-08 09:56:27,440 DEBUG: Done:	 Getting Results
2016-09-08 09:56:27,440 INFO: Classification on Fake database for View0 with DecisionTree

accuracy_score on train : 1.0
accuracy_score on test : 0.377777777778

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.377777777778
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.348837209302
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.348837209302
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.622222222222
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.377777777778
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.245415911539
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.333333333333
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.365853658537
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.376804380289
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.622222222222


 Classification took 0:00:00
2016-09-08 09:56:27,441 INFO: Done:	 Result Analysis
2016-09-08 09:56:27,452 DEBUG: Done:	 Getting Results
2016-09-08 09:56:27,452 INFO: Classification on Fake database for View0 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.377777777778

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 10, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.377777777778
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.333333333333
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.333333333333
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.622222222222
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.377777777778
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.249628898234
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.325581395349
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.341463414634
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.37481333997
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.622222222222


 Classification took 0:00:00
2016-09-08 09:56:27,452 INFO: Done:	 Result Analysis
2016-09-08 09:56:27,595 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:27,595 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:27,595 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-08 09:56:27,595 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-08 09:56:27,595 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:27,595 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:27,596 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:27,596 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:27,596 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:27,596 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:27,596 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:27,596 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:27,596 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:27,596 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:27,627 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:27,627 DEBUG: Start:	 Training
2016-09-08 09:56:27,628 DEBUG: Info:	 Time for Training: 0.0337619781494[s]
2016-09-08 09:56:27,628 DEBUG: Done:	 Training
2016-09-08 09:56:27,628 DEBUG: Start:	 Predicting
2016-09-08 09:56:27,635 DEBUG: Done:	 Predicting
2016-09-08 09:56:27,635 DEBUG: Start:	 Getting Results
2016-09-08 09:56:27,676 DEBUG: Done:	 Getting Results
2016-09-08 09:56:27,676 INFO: Classification on Fake database for View0 with KNN

accuracy_score on train : 0.609523809524
accuracy_score on test : 0.422222222222

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 42
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.609523809524
		- Score on test : 0.422222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.467532467532
		- Score on test : 0.212121212121
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.467532467532
		- Score on test : 0.212121212121
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.390476190476
		- Score on test : 0.577777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.609523809524
		- Score on test : 0.422222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.237135686833
		- Score on test : -0.218615245335
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.692307692308
		- Score on test : 0.28
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.352941176471
		- Score on test : 0.170731707317
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.602396514161
		- Score on test : 0.401692384271
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.390476190476
		- Score on test : 0.577777777778


 Classification took 0:00:00
2016-09-08 09:56:27,676 INFO: Done:	 Result Analysis
2016-09-08 09:56:27,921 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:27,921 DEBUG: Start:	 Training
2016-09-08 09:56:27,970 DEBUG: Info:	 Time for Training: 0.375941991806[s]
2016-09-08 09:56:27,970 DEBUG: Done:	 Training
2016-09-08 09:56:27,970 DEBUG: Start:	 Predicting
2016-09-08 09:56:27,976 DEBUG: Done:	 Predicting
2016-09-08 09:56:27,977 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,010 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,011 INFO: Classification on Fake database for View0 with RandomForest

accuracy_score on train : 1.0
accuracy_score on test : 0.455555555556

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 19, max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.455555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.423529411765
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.423529411765
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.544444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.455555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.0912478416452
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.409090909091
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.439024390244
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.454206072673
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.544444444444


 Classification took 0:00:00
2016-09-08 09:56:28,011 INFO: Done:	 Result Analysis
2016-09-08 09:56:28,143 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,143 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-08 09:56:28,143 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,143 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,143 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-08 09:56:28,144 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,144 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:28,144 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:28,144 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,144 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:28,144 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,144 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:28,144 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,145 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,189 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:28,189 DEBUG: Start:	 Training
2016-09-08 09:56:28,190 DEBUG: Info:	 Time for Training: 0.0481970310211[s]
2016-09-08 09:56:28,190 DEBUG: Done:	 Training
2016-09-08 09:56:28,190 DEBUG: Start:	 Predicting
2016-09-08 09:56:28,195 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:28,195 DEBUG: Start:	 Training
2016-09-08 09:56:28,214 DEBUG: Info:	 Time for Training: 0.0711450576782[s]
2016-09-08 09:56:28,214 DEBUG: Done:	 Training
2016-09-08 09:56:28,214 DEBUG: Start:	 Predicting
2016-09-08 09:56:28,216 DEBUG: Done:	 Predicting
2016-09-08 09:56:28,216 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,217 DEBUG: Done:	 Predicting
2016-09-08 09:56:28,217 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,239 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,239 INFO: Classification on Fake database for View0 with SGD

accuracy_score on train : 0.485714285714
accuracy_score on test : 0.455555555556

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : log, penalty : l1
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0
		- Score on test : 0.0
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 1.0
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.5
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444


 Classification took 0:00:00
2016-09-08 09:56:28,239 INFO: Done:	 Result Analysis
2016-09-08 09:56:28,250 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,250 INFO: Classification on Fake database for View0 with SVMLinear

accuracy_score on train : 0.52380952381
accuracy_score on test : 0.477777777778

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.52380952381
		- Score on test : 0.477777777778
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.532710280374
		- Score on test : 0.459770114943
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.532710280374
		- Score on test : 0.459770114943
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.47619047619
		- Score on test : 0.522222222222
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.52380952381
		- Score on test : 0.477777777778
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0496545019224
		- Score on test : -0.0426484477255
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.508928571429
		- Score on test : 0.434782608696
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.558823529412
		- Score on test : 0.487804878049
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.524782135076
		- Score on test : 0.478596316575
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.47619047619
		- Score on test : 0.522222222222


 Classification took 0:00:00
2016-09-08 09:56:28,250 INFO: Done:	 Result Analysis
2016-09-08 09:56:28,387 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,387 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,387 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMPoly
2016-09-08 09:56:28,387 DEBUG: ### Classification - Database:Fake Feature:View0 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMRBF
2016-09-08 09:56:28,387 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,387 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,388 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:28,388 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:28,388 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:28,388 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:28,388 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,388 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,388 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,388 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,433 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:28,433 DEBUG: Start:	 Training
2016-09-08 09:56:28,438 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:28,438 DEBUG: Start:	 Training
2016-09-08 09:56:28,450 DEBUG: Info:	 Time for Training: 0.0634729862213[s]
2016-09-08 09:56:28,450 DEBUG: Done:	 Training
2016-09-08 09:56:28,450 DEBUG: Start:	 Predicting
2016-09-08 09:56:28,455 DEBUG: Info:	 Time for Training: 0.0687439441681[s]
2016-09-08 09:56:28,455 DEBUG: Done:	 Training
2016-09-08 09:56:28,455 DEBUG: Start:	 Predicting
2016-09-08 09:56:28,455 DEBUG: Done:	 Predicting
2016-09-08 09:56:28,455 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,459 DEBUG: Done:	 Predicting
2016-09-08 09:56:28,459 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,491 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,491 INFO: Classification on Fake database for View0 with SVMRBF

accuracy_score on train : 1.0
accuracy_score on test : 0.422222222222

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.422222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.409090909091
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.409090909091
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.577777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.422222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.152357995542
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.382978723404
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.439024390244
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.423593827775
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.577777777778


 Classification took 0:00:00
2016-09-08 09:56:28,492 INFO: Done:	 Result Analysis
2016-09-08 09:56:28,505 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,505 INFO: Classification on Fake database for View0 with SVMPoly

accuracy_score on train : 1.0
accuracy_score on test : 0.533333333333

Database configuration : 
	- Database name : Fake
	- View name : View0	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.475
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.475
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.466666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.0555284586866
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.487179487179
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.463414634146
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.52762568442
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.466666666667


 Classification took 0:00:00
2016-09-08 09:56:28,505 INFO: Done:	 Result Analysis
2016-09-08 09:56:28,637 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,638 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-08 09:56:28,638 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,638 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,638 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-08 09:56:28,638 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,638 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:28,638 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:28,638 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:28,639 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,639 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:28,639 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,639 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,639 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,677 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:28,677 DEBUG: Start:	 Training
2016-09-08 09:56:28,680 DEBUG: Info:	 Time for Training: 0.0427668094635[s]
2016-09-08 09:56:28,680 DEBUG: Done:	 Training
2016-09-08 09:56:28,680 DEBUG: Start:	 Predicting
2016-09-08 09:56:28,683 DEBUG: Done:	 Predicting
2016-09-08 09:56:28,683 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,691 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:28,691 DEBUG: Start:	 Training
2016-09-08 09:56:28,696 DEBUG: Info:	 Time for Training: 0.0597720146179[s]
2016-09-08 09:56:28,696 DEBUG: Done:	 Training
2016-09-08 09:56:28,696 DEBUG: Start:	 Predicting
2016-09-08 09:56:28,699 DEBUG: Done:	 Predicting
2016-09-08 09:56:28,699 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,734 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,734 INFO: Classification on Fake database for View1 with DecisionTree

accuracy_score on train : 0.957142857143
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.957142857143
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.957746478873
		- Score on test : 0.444444444444
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.957746478873
		- Score on test : 0.444444444444
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0428571428571
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.957142857143
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.917792101918
		- Score on test : -0.104031856645
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.918918918919
		- Score on test : 0.408163265306
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.487804878049
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.958333333333
		- Score on test : 0.447984071677
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0428571428571
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-08 09:56:28,734 INFO: Done:	 Result Analysis
2016-09-08 09:56:28,741 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,741 INFO: Classification on Fake database for View1 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 10, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.390243902439
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.390243902439
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.119960179194
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.390243902439
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.390243902439
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.440019910403
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-08 09:56:28,741 INFO: Done:	 Result Analysis
2016-09-08 09:56:28,882 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,883 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-08 09:56:28,883 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,883 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:28,883 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-08 09:56:28,884 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:28,884 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:28,884 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:28,884 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:28,884 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,885 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:28,885 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:28,885 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,885 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:28,919 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:28,919 DEBUG: Start:	 Training
2016-09-08 09:56:28,920 DEBUG: Info:	 Time for Training: 0.0378148555756[s]
2016-09-08 09:56:28,920 DEBUG: Done:	 Training
2016-09-08 09:56:28,920 DEBUG: Start:	 Predicting
2016-09-08 09:56:28,927 DEBUG: Done:	 Predicting
2016-09-08 09:56:28,927 DEBUG: Start:	 Getting Results
2016-09-08 09:56:28,967 DEBUG: Done:	 Getting Results
2016-09-08 09:56:28,967 INFO: Classification on Fake database for View1 with KNN

accuracy_score on train : 0.561904761905
accuracy_score on test : 0.544444444444

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 42
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.561904761905
		- Score on test : 0.544444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.6
		- Score on test : 0.559139784946
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.6
		- Score on test : 0.559139784946
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.438095238095
		- Score on test : 0.455555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.561904761905
		- Score on test : 0.544444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.133359904768
		- Score on test : 0.104395047556
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.5390625
		- Score on test : 0.5
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.676470588235
		- Score on test : 0.634146341463
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.565087145969
		- Score on test : 0.551767048283
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.438095238095
		- Score on test : 0.455555555556


 Classification took 0:00:00
2016-09-08 09:56:28,967 INFO: Done:	 Result Analysis
2016-09-08 09:56:29,211 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:29,212 DEBUG: Start:	 Training
2016-09-08 09:56:29,260 DEBUG: Info:	 Time for Training: 0.377947092056[s]
2016-09-08 09:56:29,261 DEBUG: Done:	 Training
2016-09-08 09:56:29,261 DEBUG: Start:	 Predicting
2016-09-08 09:56:29,267 DEBUG: Done:	 Predicting
2016-09-08 09:56:29,267 DEBUG: Start:	 Getting Results
2016-09-08 09:56:29,300 DEBUG: Done:	 Getting Results
2016-09-08 09:56:29,300 INFO: Classification on Fake database for View1 with RandomForest

accuracy_score on train : 0.995238095238
accuracy_score on test : 0.522222222222

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 19, max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.522222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.995073891626
		- Score on test : 0.516853932584
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.995073891626
		- Score on test : 0.516853932584
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.477777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.522222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.990510833227
		- Score on test : 0.0506833064614
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.479166666667
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.990196078431
		- Score on test : 0.560975609756
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.995098039216
		- Score on test : 0.525385764062
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.477777777778


 Classification took 0:00:00
2016-09-08 09:56:29,300 INFO: Done:	 Result Analysis
2016-09-08 09:56:29,434 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:29,435 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-08 09:56:29,435 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:29,435 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:29,435 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-08 09:56:29,435 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:29,436 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:29,436 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:29,436 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:29,436 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:29,436 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:29,436 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:29,437 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:29,437 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:29,483 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:29,483 DEBUG: Start:	 Training
2016-09-08 09:56:29,484 DEBUG: Info:	 Time for Training: 0.0506761074066[s]
2016-09-08 09:56:29,484 DEBUG: Done:	 Training
2016-09-08 09:56:29,484 DEBUG: Start:	 Predicting
2016-09-08 09:56:29,491 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:29,491 DEBUG: Start:	 Training
2016-09-08 09:56:29,511 DEBUG: Done:	 Predicting
2016-09-08 09:56:29,511 DEBUG: Start:	 Getting Results
2016-09-08 09:56:29,520 DEBUG: Info:	 Time for Training: 0.0860919952393[s]
2016-09-08 09:56:29,520 DEBUG: Done:	 Training
2016-09-08 09:56:29,521 DEBUG: Start:	 Predicting
2016-09-08 09:56:29,526 DEBUG: Done:	 Predicting
2016-09-08 09:56:29,526 DEBUG: Start:	 Getting Results
2016-09-08 09:56:29,539 DEBUG: Done:	 Getting Results
2016-09-08 09:56:29,539 INFO: Classification on Fake database for View1 with SGD

accuracy_score on train : 0.485714285714
accuracy_score on test : 0.455555555556

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : log, penalty : l1
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0
		- Score on test : 0.0
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 1.0
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.5
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444


 Classification took 0:00:00
2016-09-08 09:56:29,540 INFO: Done:	 Result Analysis
2016-09-08 09:56:29,553 DEBUG: Done:	 Getting Results
2016-09-08 09:56:29,554 INFO: Classification on Fake database for View1 with SVMLinear

accuracy_score on train : 0.514285714286
accuracy_score on test : 0.577777777778

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.577777777778
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.547619047619
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.547619047619
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.422222222222
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.577777777778
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0294117647059
		- Score on test : 0.152357995542
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.53488372093
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.529411764706
		- Score on test : 0.560975609756
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.514705882353
		- Score on test : 0.576406172225
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.422222222222


 Classification took 0:00:00
2016-09-08 09:56:29,554 INFO: Done:	 Result Analysis
2016-09-08 09:56:29,683 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:29,684 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:29,684 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMRBF
2016-09-08 09:56:29,684 DEBUG: ### Classification - Database:Fake Feature:View1 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMPoly
2016-09-08 09:56:29,684 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:29,684 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:29,685 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:29,685 DEBUG: Info:	 Shape X_train:(210, 19), Length of y_train:210
2016-09-08 09:56:29,685 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:29,685 DEBUG: Info:	 Shape X_test:(90, 19), Length of y_test:90
2016-09-08 09:56:29,686 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:29,686 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:29,686 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:29,686 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:29,756 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:29,756 DEBUG: Start:	 Training
2016-09-08 09:56:29,762 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:29,762 DEBUG: Start:	 Training
2016-09-08 09:56:29,781 DEBUG: Info:	 Time for Training: 0.0979940891266[s]
2016-09-08 09:56:29,781 DEBUG: Done:	 Training
2016-09-08 09:56:29,781 DEBUG: Start:	 Predicting
2016-09-08 09:56:29,788 DEBUG: Info:	 Time for Training: 0.105060100555[s]
2016-09-08 09:56:29,788 DEBUG: Done:	 Training
2016-09-08 09:56:29,788 DEBUG: Start:	 Predicting
2016-09-08 09:56:29,790 DEBUG: Done:	 Predicting
2016-09-08 09:56:29,790 DEBUG: Start:	 Getting Results
2016-09-08 09:56:29,793 DEBUG: Done:	 Predicting
2016-09-08 09:56:29,794 DEBUG: Start:	 Getting Results
2016-09-08 09:56:29,827 DEBUG: Done:	 Getting Results
2016-09-08 09:56:29,827 DEBUG: Done:	 Getting Results
2016-09-08 09:56:29,827 INFO: Classification on Fake database for View1 with SVMPoly

accuracy_score on train : 1.0
accuracy_score on test : 0.522222222222

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.522222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.394366197183
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.394366197183
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.477777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.522222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.0157759322964
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.466666666667
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.341463414634
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.507466401195
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.477777777778


 Classification took 0:00:00
2016-09-08 09:56:29,827 INFO: Classification on Fake database for View1 with SVMRBF

accuracy_score on train : 1.0
accuracy_score on test : 0.511111111111

Database configuration : 
	- Database name : Fake
	- View name : View1	 View shape : (300, 19)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.511111111111
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.333333333333
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.333333333333
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.488888888889
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.511111111111
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.0193709711057
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.44
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.268292682927
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.491289198606
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.488888888889


 Classification took 0:00:00
2016-09-08 09:56:29,827 INFO: Done:	 Result Analysis
2016-09-08 09:56:29,828 INFO: Done:	 Result Analysis
2016-09-08 09:56:29,934 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:29,934 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-08 09:56:29,934 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:29,934 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:29,935 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-08 09:56:29,935 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:29,935 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:29,935 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:29,936 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:29,936 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:29,936 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:29,936 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:29,936 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:29,936 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:29,974 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:29,974 DEBUG: Start:	 Training
2016-09-08 09:56:29,976 DEBUG: Info:	 Time for Training: 0.0422959327698[s]
2016-09-08 09:56:29,976 DEBUG: Done:	 Training
2016-09-08 09:56:29,976 DEBUG: Start:	 Predicting
2016-09-08 09:56:29,979 DEBUG: Done:	 Predicting
2016-09-08 09:56:29,979 DEBUG: Start:	 Getting Results
2016-09-08 09:56:29,988 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:29,988 DEBUG: Start:	 Training
2016-09-08 09:56:29,994 DEBUG: Info:	 Time for Training: 0.0606279373169[s]
2016-09-08 09:56:29,994 DEBUG: Done:	 Training
2016-09-08 09:56:29,994 DEBUG: Start:	 Predicting
2016-09-08 09:56:29,997 DEBUG: Done:	 Predicting
2016-09-08 09:56:29,997 DEBUG: Start:	 Getting Results
2016-09-08 09:56:30,029 DEBUG: Done:	 Getting Results
2016-09-08 09:56:30,029 INFO: Classification on Fake database for View2 with DecisionTree

accuracy_score on train : 1.0
accuracy_score on test : 0.433333333333

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.433333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.385542168675
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.385542168675
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.566666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.433333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.140124435511
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.380952380952
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.390243902439
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.429815828771
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.566666666667


 Classification took 0:00:00
2016-09-08 09:56:30,029 INFO: Done:	 Result Analysis
2016-09-08 09:56:30,043 DEBUG: Done:	 Getting Results
2016-09-08 09:56:30,043 INFO: Classification on Fake database for View2 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.444444444444

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 10, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.375
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.375
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.444444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.124563839757
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.384615384615
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.365853658537
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.438028870085
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.555555555556


 Classification took 0:00:00
2016-09-08 09:56:30,044 INFO: Done:	 Result Analysis
2016-09-08 09:56:30,185 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:30,186 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-08 09:56:30,186 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:30,186 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:30,186 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-08 09:56:30,186 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:30,187 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:30,187 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:30,187 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:30,187 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:30,187 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:30,187 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:30,187 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:30,188 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:30,236 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:30,237 DEBUG: Start:	 Training
2016-09-08 09:56:30,238 DEBUG: Info:	 Time for Training: 0.0538048744202[s]
2016-09-08 09:56:30,238 DEBUG: Done:	 Training
2016-09-08 09:56:30,238 DEBUG: Start:	 Predicting
2016-09-08 09:56:30,249 DEBUG: Done:	 Predicting
2016-09-08 09:56:30,249 DEBUG: Start:	 Getting Results
2016-09-08 09:56:30,288 DEBUG: Done:	 Getting Results
2016-09-08 09:56:30,288 INFO: Classification on Fake database for View2 with KNN

accuracy_score on train : 0.542857142857
accuracy_score on test : 0.588888888889

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 42
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.542857142857
		- Score on test : 0.588888888889
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.323943661972
		- Score on test : 0.327272727273
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.323943661972
		- Score on test : 0.327272727273
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.457142857143
		- Score on test : 0.411111111111
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.542857142857
		- Score on test : 0.588888888889
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0866552427925
		- Score on test : 0.161417724438
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.575
		- Score on test : 0.642857142857
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.225490196078
		- Score on test : 0.219512195122
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.534041394336
		- Score on test : 0.558735689398
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.457142857143
		- Score on test : 0.411111111111


 Classification took 0:00:00
2016-09-08 09:56:30,289 INFO: Done:	 Result Analysis
2016-09-08 09:56:30,545 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:30,545 DEBUG: Start:	 Training
2016-09-08 09:56:30,594 DEBUG: Info:	 Time for Training: 0.40927195549[s]
2016-09-08 09:56:30,594 DEBUG: Done:	 Training
2016-09-08 09:56:30,594 DEBUG: Start:	 Predicting
2016-09-08 09:56:30,600 DEBUG: Done:	 Predicting
2016-09-08 09:56:30,600 DEBUG: Start:	 Getting Results
2016-09-08 09:56:30,633 DEBUG: Done:	 Getting Results
2016-09-08 09:56:30,633 INFO: Classification on Fake database for View2 with RandomForest

accuracy_score on train : 1.0
accuracy_score on test : 0.588888888889

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 19, max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.588888888889
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.55421686747
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.55421686747
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.411111111111
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.588888888889
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.172919516163
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.547619047619
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.560975609756
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.586610253858
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.411111111111


 Classification took 0:00:00
2016-09-08 09:56:30,633 INFO: Done:	 Result Analysis
2016-09-08 09:56:30,727 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:30,727 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-08 09:56:30,727 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:30,727 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:30,728 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-08 09:56:30,728 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:30,728 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:30,728 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:30,728 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:30,728 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:30,728 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:30,728 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:30,729 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:30,729 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:30,773 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:30,773 DEBUG: Start:	 Training
2016-09-08 09:56:30,774 DEBUG: Info:	 Time for Training: 0.0476858615875[s]
2016-09-08 09:56:30,774 DEBUG: Done:	 Training
2016-09-08 09:56:30,774 DEBUG: Start:	 Predicting
2016-09-08 09:56:30,779 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:30,779 DEBUG: Start:	 Training
2016-09-08 09:56:30,796 DEBUG: Done:	 Predicting
2016-09-08 09:56:30,796 DEBUG: Start:	 Getting Results
2016-09-08 09:56:30,800 DEBUG: Info:	 Time for Training: 0.0737199783325[s]
2016-09-08 09:56:30,800 DEBUG: Done:	 Training
2016-09-08 09:56:30,801 DEBUG: Start:	 Predicting
2016-09-08 09:56:30,804 DEBUG: Done:	 Predicting
2016-09-08 09:56:30,804 DEBUG: Start:	 Getting Results
2016-09-08 09:56:30,819 DEBUG: Done:	 Getting Results
2016-09-08 09:56:30,819 INFO: Classification on Fake database for View2 with SGD

accuracy_score on train : 0.485714285714
accuracy_score on test : 0.455555555556

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : log, penalty : l1
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0
		- Score on test : 0.0
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 1.0
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.5
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444


 Classification took 0:00:00
2016-09-08 09:56:30,819 INFO: Done:	 Result Analysis
2016-09-08 09:56:30,833 DEBUG: Done:	 Getting Results
2016-09-08 09:56:30,833 INFO: Classification on Fake database for View2 with SVMLinear

accuracy_score on train : 0.533333333333
accuracy_score on test : 0.588888888889

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.533333333333
		- Score on test : 0.588888888889
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.514851485149
		- Score on test : 0.543209876543
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.514851485149
		- Score on test : 0.543209876543
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.466666666667
		- Score on test : 0.411111111111
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.533333333333
		- Score on test : 0.588888888889
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0654069940168
		- Score on test : 0.169618786115
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.52
		- Score on test : 0.55
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.509803921569
		- Score on test : 0.536585365854
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.532679738562
		- Score on test : 0.584619213539
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.466666666667
		- Score on test : 0.411111111111


 Classification took 0:00:00
2016-09-08 09:56:30,833 INFO: Done:	 Result Analysis
2016-09-08 09:56:30,973 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:30,973 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:30,974 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMPoly
2016-09-08 09:56:30,974 DEBUG: ### Classification - Database:Fake Feature:View2 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMRBF
2016-09-08 09:56:30,974 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:30,974 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:30,974 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:30,974 DEBUG: Info:	 Shape X_train:(210, 20), Length of y_train:210
2016-09-08 09:56:30,974 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:30,974 DEBUG: Info:	 Shape X_test:(90, 20), Length of y_test:90
2016-09-08 09:56:30,975 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:30,975 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:30,975 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:30,975 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:31,020 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:31,020 DEBUG: Start:	 Training
2016-09-08 09:56:31,024 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:31,024 DEBUG: Start:	 Training
2016-09-08 09:56:31,039 DEBUG: Info:	 Time for Training: 0.0657398700714[s]
2016-09-08 09:56:31,039 DEBUG: Done:	 Training
2016-09-08 09:56:31,039 DEBUG: Start:	 Predicting
2016-09-08 09:56:31,043 DEBUG: Info:	 Time for Training: 0.0697932243347[s]
2016-09-08 09:56:31,043 DEBUG: Done:	 Training
2016-09-08 09:56:31,043 DEBUG: Start:	 Predicting
2016-09-08 09:56:31,045 DEBUG: Done:	 Predicting
2016-09-08 09:56:31,045 DEBUG: Start:	 Getting Results
2016-09-08 09:56:31,047 DEBUG: Done:	 Predicting
2016-09-08 09:56:31,047 DEBUG: Start:	 Getting Results
2016-09-08 09:56:31,077 DEBUG: Done:	 Getting Results
2016-09-08 09:56:31,077 INFO: Classification on Fake database for View2 with SVMPoly

accuracy_score on train : 1.0
accuracy_score on test : 0.455555555556

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.455555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.588235294118
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.588235294118
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.544444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.455555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : -0.0350036509618
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.448717948718
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.853658536585
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.488053758089
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.544444444444


 Classification took 0:00:00
2016-09-08 09:56:31,077 INFO: Done:	 Result Analysis
2016-09-08 09:56:31,077 DEBUG: Done:	 Getting Results
2016-09-08 09:56:31,077 INFO: Classification on Fake database for View2 with SVMRBF

accuracy_score on train : 1.0
accuracy_score on test : 0.544444444444

Database configuration : 
	- Database name : Fake
	- View name : View2	 View shape : (300, 20)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.544444444444
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.453333333333
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.453333333333
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.455555555556
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.544444444444
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.0695369227879
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.414634146341
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533847685416
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.455555555556


 Classification took 0:00:00
2016-09-08 09:56:31,078 INFO: Done:	 Result Analysis
2016-09-08 09:56:31,220 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:31,220 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:31,220 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : Adaboost
2016-09-08 09:56:31,220 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : DecisionTree
2016-09-08 09:56:31,221 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:31,221 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:31,221 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:31,221 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:31,221 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:31,221 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:31,221 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:31,222 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:31,222 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:31,222 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:31,257 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:31,257 DEBUG: Start:	 Training
2016-09-08 09:56:31,259 DEBUG: Info:	 Time for Training: 0.0391139984131[s]
2016-09-08 09:56:31,259 DEBUG: Done:	 Training
2016-09-08 09:56:31,259 DEBUG: Start:	 Predicting
2016-09-08 09:56:31,262 DEBUG: Done:	 Predicting
2016-09-08 09:56:31,262 DEBUG: Start:	 Getting Results
2016-09-08 09:56:31,270 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:31,270 DEBUG: Start:	 Training
2016-09-08 09:56:31,274 DEBUG: Info:	 Time for Training: 0.0547120571136[s]
2016-09-08 09:56:31,275 DEBUG: Done:	 Training
2016-09-08 09:56:31,275 DEBUG: Start:	 Predicting
2016-09-08 09:56:31,277 DEBUG: Done:	 Predicting
2016-09-08 09:56:31,277 DEBUG: Start:	 Getting Results
2016-09-08 09:56:31,302 DEBUG: Done:	 Getting Results
2016-09-08 09:56:31,302 INFO: Classification on Fake database for View3 with DecisionTree

accuracy_score on train : 0.985714285714
accuracy_score on test : 0.522222222222

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Decision Tree with max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.985714285714
		- Score on test : 0.522222222222
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.985507246377
		- Score on test : 0.516853932584
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.985507246377
		- Score on test : 0.516853932584
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0142857142857
		- Score on test : 0.477777777778
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.985714285714
		- Score on test : 0.522222222222
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.971825315808
		- Score on test : 0.0506833064614
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.971428571429
		- Score on test : 0.479166666667
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.560975609756
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.986111111111
		- Score on test : 0.525385764062
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0142857142857
		- Score on test : 0.477777777778


 Classification took 0:00:00
2016-09-08 09:56:31,303 INFO: Done:	 Result Analysis
2016-09-08 09:56:31,313 DEBUG: Done:	 Getting Results
2016-09-08 09:56:31,314 INFO: Classification on Fake database for View3 with Adaboost

accuracy_score on train : 1.0
accuracy_score on test : 0.533333333333

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Adaboost with num_esimators : 10, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533333333333
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.511627906977
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.511627906977
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.466666666667
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533333333333
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.066931612238
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.488888888889
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.536585365854
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.533598805376
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.466666666667


 Classification took 0:00:00
2016-09-08 09:56:31,314 INFO: Done:	 Result Analysis
2016-09-08 09:56:31,468 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:31,468 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : KNN
2016-09-08 09:56:31,468 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:31,468 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:31,469 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : RandomForest
2016-09-08 09:56:31,469 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:31,469 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:31,469 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:31,469 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:31,469 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:31,469 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:31,469 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:31,470 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:31,470 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:31,503 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:31,503 DEBUG: Start:	 Training
2016-09-08 09:56:31,504 DEBUG: Info:	 Time for Training: 0.0364670753479[s]
2016-09-08 09:56:31,504 DEBUG: Done:	 Training
2016-09-08 09:56:31,504 DEBUG: Start:	 Predicting
2016-09-08 09:56:31,511 DEBUG: Done:	 Predicting
2016-09-08 09:56:31,511 DEBUG: Start:	 Getting Results
2016-09-08 09:56:31,565 DEBUG: Done:	 Getting Results
2016-09-08 09:56:31,565 INFO: Classification on Fake database for View3 with KNN

accuracy_score on train : 0.533333333333
accuracy_score on test : 0.588888888889

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 42
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.533333333333
		- Score on test : 0.588888888889
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.505050505051
		- Score on test : 0.543209876543
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.505050505051
		- Score on test : 0.543209876543
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.466666666667
		- Score on test : 0.411111111111
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.533333333333
		- Score on test : 0.588888888889
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0644812208979
		- Score on test : 0.169618786115
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.520833333333
		- Score on test : 0.55
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.490196078431
		- Score on test : 0.536585365854
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.532135076253
		- Score on test : 0.584619213539
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.466666666667
		- Score on test : 0.411111111111


 Classification took 0:00:00
2016-09-08 09:56:31,565 INFO: Done:	 Result Analysis
2016-09-08 09:56:31,800 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:31,800 DEBUG: Start:	 Training
2016-09-08 09:56:31,848 DEBUG: Info:	 Time for Training: 0.380412101746[s]
2016-09-08 09:56:31,848 DEBUG: Done:	 Training
2016-09-08 09:56:31,848 DEBUG: Start:	 Predicting
2016-09-08 09:56:31,855 DEBUG: Done:	 Predicting
2016-09-08 09:56:31,855 DEBUG: Start:	 Getting Results
2016-09-08 09:56:31,882 DEBUG: Done:	 Getting Results
2016-09-08 09:56:31,882 INFO: Classification on Fake database for View3 with RandomForest

accuracy_score on train : 0.995238095238
accuracy_score on test : 0.511111111111

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- Random Forest with num_esimators : 19, max_depth : 10
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.511111111111
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.99512195122
		- Score on test : 0.488372093023
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.99512195122
		- Score on test : 0.488372093023
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.488888888889
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.995238095238
		- Score on test : 0.511111111111
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.990515975943
		- Score on test : 0.0223105374127
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.990291262136
		- Score on test : 0.466666666667
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.512195121951
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.99537037037
		- Score on test : 0.511199601792
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0047619047619
		- Score on test : 0.488888888889


 Classification took 0:00:00
2016-09-08 09:56:31,882 INFO: Done:	 Result Analysis
2016-09-08 09:56:32,016 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:32,016 DEBUG: ### Main Programm for Classification MonoView
2016-09-08 09:56:32,016 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SVMLinear
2016-09-08 09:56:32,016 DEBUG: ### Classification - Database:Fake Feature:View3 train_size:0.7, CrossValidation k-folds:5, cores:1, algorithm : SGD
2016-09-08 09:56:32,016 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:32,016 DEBUG: Start:	 Determine Train/Test split
2016-09-08 09:56:32,017 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:32,017 DEBUG: Info:	 Shape X_train:(210, 12), Length of y_train:210
2016-09-08 09:56:32,017 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:32,017 DEBUG: Info:	 Shape X_test:(90, 12), Length of y_test:90
2016-09-08 09:56:32,017 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:32,017 DEBUG: Done:	 Determine Train/Test split
2016-09-08 09:56:32,017 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:32,017 DEBUG: Start:	 RandomSearch best settings with 1 iterations
2016-09-08 09:56:32,062 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:32,062 DEBUG: Start:	 Training
2016-09-08 09:56:32,062 DEBUG: Info:	 Time for Training: 0.04727602005[s]
2016-09-08 09:56:32,063 DEBUG: Done:	 Training
2016-09-08 09:56:32,063 DEBUG: Start:	 Predicting
2016-09-08 09:56:32,066 DEBUG: Done:	 RandomSearch best settings
2016-09-08 09:56:32,067 DEBUG: Start:	 Training
2016-09-08 09:56:32,075 DEBUG: Done:	 Predicting
2016-09-08 09:56:32,076 DEBUG: Start:	 Getting Results
2016-09-08 09:56:32,089 DEBUG: Info:	 Time for Training: 0.0742139816284[s]
2016-09-08 09:56:32,090 DEBUG: Done:	 Training
2016-09-08 09:56:32,090 DEBUG: Start:	 Predicting
2016-09-08 09:56:32,093 DEBUG: Done:	 Predicting
2016-09-08 09:56:32,093 DEBUG: Start:	 Getting Results
2016-09-08 09:56:32,100 DEBUG: Done:	 Getting Results
2016-09-08 09:56:32,100 INFO: Classification on Fake database for View3 with SGD

accuracy_score on train : 0.485714285714
accuracy_score on test : 0.455555555556

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SGDClassifier with loss : log, penalty : l1
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.653846153846
		- Score on test : 0.625954198473
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.0
		- Score on test : 0.0
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.485714285714
		- Score on test : 0.455555555556
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 1.0
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.5
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.514285714286
		- Score on test : 0.544444444444


 Classification took 0:00:00
2016-09-08 09:56:32,100 INFO: Done:	 Result Analysis
2016-09-08 09:56:32,122 DEBUG: Done:	 Getting Results
2016-09-08 09:56:32,122 INFO: Classification on Fake database for View3 with SVMLinear

accuracy_score on train : 0.552380952381
accuracy_score on test : 0.566666666667

Database configuration : 
	- Database name : Fake
	- View name : View3	 View shape : (300, 12)
	- Learning Rate : 0.7
	- Labels used : Non, Oui
	- Number of cross validation folds : 5

Classifier configuration : 
	- SVM Linear with C : 8627
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 1 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.552380952381
		- Score on test : 0.566666666667
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, micro as average (higher is better) : 
		- Score on train : 0.520408163265
		- Score on test : 0.506329113924
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, micro as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.520408163265
		- Score on test : 0.506329113924
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.447619047619
		- Score on test : 0.433333333333
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.552380952381
		- Score on test : 0.566666666667
	For Log loss using None as sample_weights, 1e-15 as eps (lower is better) : 
		- Score on train : nan
		- Score on test : nan
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.10237359911
		- Score on test : 0.121459622637
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.542553191489
		- Score on test : 0.526315789474
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.5
		- Score on test : 0.487804878049
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.550925925926
		- Score on test : 0.560228969637
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.447619047619
		- Score on test : 0.433333333333


 Classification took 0:00:00
2016-09-08 09:56:32,122 INFO: Done:	 Result Analysis
2016-09-08 09:56:32,424 INFO: ### Main Programm for Multiview Classification
2016-09-08 09:56:32,425 INFO: ### Classification - Database : Fake ; Views : Methyl, MiRNA_, RNASeq, Clinic ; Algorithm : Fusion ; Cores : 1
2016-09-08 09:56:32,426 INFO: Info:	 Shape of View0 :(300, 12)
2016-09-08 09:56:32,426 INFO: Info:	 Shape of View1 :(300, 19)
2016-09-08 09:56:32,427 INFO: Info:	 Shape of View2 :(300, 20)
2016-09-08 09:56:32,427 INFO: Info:	 Shape of View3 :(300, 12)
2016-09-08 09:56:32,428 INFO: Done:	 Read Database Files
2016-09-08 09:56:32,428 INFO: Start:	 Determine validation split for ratio 0.7
2016-09-08 09:56:32,431 INFO: ### Main Programm for Multiview Classification
2016-09-08 09:56:32,432 INFO: ### Classification - Database : Fake ; Views : Methyl, MiRNA_, RNASeq, Clinic ; Algorithm : Fusion ; Cores : 1
2016-09-08 09:56:32,432 INFO: Info:	 Shape of View0 :(300, 12)
2016-09-08 09:56:32,433 INFO: Done:	 Determine validation split
2016-09-08 09:56:32,433 INFO: Start:	 Determine 5 folds
2016-09-08 09:56:32,433 INFO: Info:	 Shape of View1 :(300, 19)
2016-09-08 09:56:32,434 INFO: Info:	 Shape of View2 :(300, 20)
2016-09-08 09:56:32,434 INFO: Info:	 Shape of View3 :(300, 12)
2016-09-08 09:56:32,434 INFO: Done:	 Read Database Files
2016-09-08 09:56:32,434 INFO: Start:	 Determine validation split for ratio 0.7
2016-09-08 09:56:32,439 INFO: Done:	 Determine validation split
2016-09-08 09:56:32,439 INFO: Start:	 Determine 5 folds
2016-09-08 09:56:32,445 INFO: Info:	 Length of Learning Sets: 169
2016-09-08 09:56:32,445 INFO: Info:	 Length of Testing Sets: 42
2016-09-08 09:56:32,445 INFO: Info:	 Length of Validation Set: 89
2016-09-08 09:56:32,445 INFO: Done:	 Determine folds
2016-09-08 09:56:32,445 INFO: Start:	 Learning with Fusion and 5 folds
2016-09-08 09:56:32,445 INFO: Start:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:32,445 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,448 INFO: Info:	 Length of Learning Sets: 169
2016-09-08 09:56:32,448 INFO: Info:	 Length of Testing Sets: 42
2016-09-08 09:56:32,449 INFO: Info:	 Length of Validation Set: 89
2016-09-08 09:56:32,449 INFO: Done:	 Determine folds
2016-09-08 09:56:32,449 INFO: Start:	 Learning with Fusion and 5 folds
2016-09-08 09:56:32,449 INFO: Start:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:32,449 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,500 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,501 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,504 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,504 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,551 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,551 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,556 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,556 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,600 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,600 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,607 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,607 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:32,651 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,660 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:32,717 INFO: Done:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:32,717 INFO: Start:	 Classification
2016-09-08 09:56:32,717 INFO: 	Start:	 Fold number 1
2016-09-08 09:56:32,745 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,751 INFO: Done:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:32,751 INFO: Start:	 Classification
2016-09-08 09:56:32,751 INFO: 	Start:	 Fold number 1
2016-09-08 09:56:32,771 INFO: 	Done: 	 Fold number 1
2016-09-08 09:56:32,772 INFO: 	Start:	 Fold number 2
2016-09-08 09:56:32,778 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,798 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,824 INFO: 	Done: 	 Fold number 2
2016-09-08 09:56:32,824 INFO: 	Start:	 Fold number 3
2016-09-08 09:56:32,847 INFO: 	Done: 	 Fold number 1
2016-09-08 09:56:32,848 INFO: 	Start:	 Fold number 2
2016-09-08 09:56:32,850 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,874 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,876 INFO: 	Done: 	 Fold number 3
2016-09-08 09:56:32,877 INFO: 	Start:	 Fold number 4
2016-09-08 09:56:32,903 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,928 INFO: 	Done: 	 Fold number 4
2016-09-08 09:56:32,928 INFO: 	Start:	 Fold number 5
2016-09-08 09:56:32,944 INFO: 	Done: 	 Fold number 2
2016-09-08 09:56:32,944 INFO: 	Start:	 Fold number 3
2016-09-08 09:56:32,955 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,971 INFO: 	Start: 	 Classification
2016-09-08 09:56:32,981 INFO: 	Done: 	 Fold number 5
2016-09-08 09:56:32,981 INFO: Done:	 Classification
2016-09-08 09:56:32,981 INFO: Info:	 Time for Classification: 0[s]
2016-09-08 09:56:32,981 INFO: Start:	 Result Analysis for Fusion
2016-09-08 09:56:33,041 INFO: 	Done: 	 Fold number 3
2016-09-08 09:56:33,041 INFO: 	Start:	 Fold number 4
2016-09-08 09:56:33,068 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,109 INFO: 		Result for Multiview classification with LateFusion

Average accuracy :
	-On Train : 64.9704142012
	-On Test : 42.380952381
	-On Validation : 48.7640449438

Dataset info :
	-Database name : Fake
	-Labels : Methyl, MiRNA_, RNASeq, Clinic
	-Views : Methyl, MiRNA_, RNASeq, Clinic
	-5 folds

Classification configuration : 
	-Algorithm used : LateFusion with Bayesian Inference using a weight for each view : 0.241839908393, 0.362121620258, 0.0533308084229, 0.342707662926
	-With monoview classifiers : 
		- SGDClassifier with loss : modified_huber, penalty : elasticnet
		- SGDClassifier with loss : log, penalty : l2
		- SGDClassifier with loss : modified_huber, penalty : l2
		- SGDClassifier with loss : log, penalty : elasticnet

Computation time on 1 cores : 
	Database extraction time : 0:00:00
	                         Learn     Prediction
	         Fold 1        0:00:00        0:00:00
	         Fold 2        0:00:00        0:00:00
	         Fold 3        0:00:00        0:00:00
	         Fold 4        0:00:00        0:00:00
	         Fold 5        0:00:00        0:00:00
	          Total        0:00:02        0:00:00
	So a total classification time of 0:00:00.


2016-09-08 09:56:33,109 INFO: Done:	 Result Analysis
2016-09-08 09:56:33,137 INFO: 	Done: 	 Fold number 4
2016-09-08 09:56:33,137 INFO: 	Start:	 Fold number 5
2016-09-08 09:56:33,163 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,229 INFO: 	Done: 	 Fold number 5
2016-09-08 09:56:33,229 INFO: Done:	 Classification
2016-09-08 09:56:33,229 INFO: Info:	 Time for Classification: 0[s]
2016-09-08 09:56:33,229 INFO: Start:	 Result Analysis for Fusion
2016-09-08 09:56:33,351 INFO: 		Result for Multiview classification with LateFusion

Average accuracy :
	-On Train : 62.2485207101
	-On Test : 47.619047619
	-On Validation : 52.1348314607

Dataset info :
	-Database name : Fake
	-Labels : Methyl, MiRNA_, RNASeq, Clinic
	-Views : Methyl, MiRNA_, RNASeq, Clinic
	-5 folds

Classification configuration : 
	-Algorithm used : LateFusion with Majority Voting 
	-With monoview classifiers : 
		- SGDClassifier with loss : modified_huber, penalty : elasticnet
		- SGDClassifier with loss : log, penalty : l2
		- SGDClassifier with loss : modified_huber, penalty : l2
		- SGDClassifier with loss : log, penalty : elasticnet

Computation time on 1 cores : 
	Database extraction time : 0:00:00
	                         Learn     Prediction
	         Fold 1        0:00:00        0:00:00
	         Fold 2        0:00:00        0:00:00
	         Fold 3        0:00:00        0:00:00
	         Fold 4        0:00:00        0:00:00
	         Fold 5        0:00:00        0:00:00
	          Total        0:00:02        0:00:00
	So a total classification time of 0:00:00.


2016-09-08 09:56:33,351 INFO: Done:	 Result Analysis
2016-09-08 09:56:33,471 INFO: ### Main Programm for Multiview Classification
2016-09-08 09:56:33,471 INFO: ### Classification - Database : Fake ; Views : Methyl, MiRNA_, RNASeq, Clinic ; Algorithm : Fusion ; Cores : 1
2016-09-08 09:56:33,472 INFO: Info:	 Shape of View0 :(300, 12)
2016-09-08 09:56:33,472 INFO: Info:	 Shape of View1 :(300, 19)
2016-09-08 09:56:33,473 INFO: Info:	 Shape of View2 :(300, 20)
2016-09-08 09:56:33,473 INFO: Info:	 Shape of View3 :(300, 12)
2016-09-08 09:56:33,473 INFO: Done:	 Read Database Files
2016-09-08 09:56:33,474 INFO: Start:	 Determine validation split for ratio 0.7
2016-09-08 09:56:33,478 INFO: ### Main Programm for Multiview Classification
2016-09-08 09:56:33,478 INFO: ### Classification - Database : Fake ; Views : Methyl, MiRNA_, RNASeq, Clinic ; Algorithm : Fusion ; Cores : 1
2016-09-08 09:56:33,479 INFO: Info:	 Shape of View0 :(300, 12)
2016-09-08 09:56:33,480 INFO: Info:	 Shape of View1 :(300, 19)
2016-09-08 09:56:33,481 INFO: Done:	 Determine validation split
2016-09-08 09:56:33,481 INFO: Start:	 Determine 5 folds
2016-09-08 09:56:33,481 INFO: Info:	 Shape of View2 :(300, 20)
2016-09-08 09:56:33,482 INFO: Info:	 Shape of View3 :(300, 12)
2016-09-08 09:56:33,482 INFO: Done:	 Read Database Files
2016-09-08 09:56:33,482 INFO: Start:	 Determine validation split for ratio 0.7
2016-09-08 09:56:33,485 INFO: Done:	 Determine validation split
2016-09-08 09:56:33,485 INFO: Start:	 Determine 5 folds
2016-09-08 09:56:33,488 INFO: Info:	 Length of Learning Sets: 169
2016-09-08 09:56:33,489 INFO: Info:	 Length of Testing Sets: 42
2016-09-08 09:56:33,489 INFO: Info:	 Length of Validation Set: 89
2016-09-08 09:56:33,489 INFO: Done:	 Determine folds
2016-09-08 09:56:33,489 INFO: Start:	 Learning with Fusion and 5 folds
2016-09-08 09:56:33,489 INFO: Start:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:33,489 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,495 INFO: Info:	 Length of Learning Sets: 169
2016-09-08 09:56:33,495 INFO: Info:	 Length of Testing Sets: 42
2016-09-08 09:56:33,495 INFO: Info:	 Length of Validation Set: 89
2016-09-08 09:56:33,495 INFO: Done:	 Determine folds
2016-09-08 09:56:33,496 INFO: Start:	 Learning with Fusion and 5 folds
2016-09-08 09:56:33,496 INFO: Start:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:33,496 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,544 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,545 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,550 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,550 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,596 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,596 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,600 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,601 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,646 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,646 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,650 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,650 DEBUG: 	Start:	 Random search for SGD with 1 iterations
2016-09-08 09:56:33,698 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,698 INFO: Done:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:33,698 INFO: Start:	 Classification
2016-09-08 09:56:33,699 INFO: 	Start:	 Fold number 1
2016-09-08 09:56:33,702 DEBUG: 	Done:	 Random search for SGD
2016-09-08 09:56:33,760 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,767 INFO: Done:	 Randomsearching best settings for monoview classifiers
2016-09-08 09:56:33,767 INFO: Start:	 Classification
2016-09-08 09:56:33,767 INFO: 	Start:	 Fold number 1
2016-09-08 09:56:33,790 INFO: 	Done: 	 Fold number 1
2016-09-08 09:56:33,790 INFO: 	Start:	 Fold number 2
2016-09-08 09:56:33,794 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,825 INFO: 	Done: 	 Fold number 1
2016-09-08 09:56:33,825 INFO: 	Start:	 Fold number 2
2016-09-08 09:56:33,835 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,851 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,864 INFO: 	Done: 	 Fold number 2
2016-09-08 09:56:33,865 INFO: 	Start:	 Fold number 3
2016-09-08 09:56:33,882 INFO: 	Done: 	 Fold number 2
2016-09-08 09:56:33,882 INFO: 	Start:	 Fold number 3
2016-09-08 09:56:33,908 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,909 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,938 INFO: 	Done: 	 Fold number 3
2016-09-08 09:56:33,938 INFO: 	Start:	 Fold number 4
2016-09-08 09:56:33,938 INFO: 	Done: 	 Fold number 3
2016-09-08 09:56:33,939 INFO: 	Start:	 Fold number 4
2016-09-08 09:56:33,965 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,984 INFO: 	Start: 	 Classification
2016-09-08 09:56:33,997 INFO: 	Done: 	 Fold number 4
2016-09-08 09:56:33,997 INFO: 	Start:	 Fold number 5
2016-09-08 09:56:34,014 INFO: 	Done: 	 Fold number 4
2016-09-08 09:56:34,014 INFO: 	Start:	 Fold number 5
2016-09-08 09:56:34,024 INFO: 	Start: 	 Classification
2016-09-08 09:56:34,054 INFO: 	Done: 	 Fold number 5
2016-09-08 09:56:34,054 INFO: Done:	 Classification
2016-09-08 09:56:34,054 INFO: Info:	 Time for Classification: 0[s]
2016-09-08 09:56:34,055 INFO: Start:	 Result Analysis for Fusion
2016-09-08 09:56:34,059 INFO: 	Start: 	 Classification
2016-09-08 09:56:34,089 INFO: 	Done: 	 Fold number 5
2016-09-08 09:56:34,089 INFO: Done:	 Classification
2016-09-08 09:56:34,089 INFO: Info:	 Time for Classification: 0[s]
2016-09-08 09:56:34,089 INFO: Start:	 Result Analysis for Fusion
2016-09-08 09:56:34,221 INFO: 		Result for Multiview classification with LateFusion

Average accuracy :
	-On Train : 63.4319526627
	-On Test : 47.619047619
	-On Validation : 44.2696629213

Dataset info :
	-Database name : Fake
	-Labels : Methyl, MiRNA_, RNASeq, Clinic
	-Views : Methyl, MiRNA_, RNASeq, Clinic
	-5 folds

Classification configuration : 
	-Algorithm used : LateFusion with SVM for linear 
	-With monoview classifiers : 
		- SGDClassifier with loss : modified_huber, penalty : elasticnet
		- SGDClassifier with loss : log, penalty : l2
		- SGDClassifier with loss : modified_huber, penalty : l2
		- SGDClassifier with loss : log, penalty : elasticnet

Computation time on 1 cores : 
	Database extraction time : 0:00:00
	                         Learn     Prediction
	         Fold 1        0:00:00        0:00:00
	         Fold 2        0:00:00        0:00:00
	         Fold 3        0:00:00        0:00:00
	         Fold 4        0:00:00        0:00:00
	         Fold 5        0:00:00        0:00:00
	          Total        0:00:02        0:00:00
	So a total classification time of 0:00:00.


2016-09-08 09:56:34,222 INFO: Done:	 Result Analysis
