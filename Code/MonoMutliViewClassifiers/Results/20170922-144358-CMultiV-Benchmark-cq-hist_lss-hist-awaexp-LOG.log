2017-09-22 14:44:04,106 DEBUG: Info:	 Enough copies of the dataset are already available
2017-09-22 14:44:04,108 INFO: Start:	 Finding all available mono- & multiview algorithms
2017-09-22 14:44:04,179 DEBUG: Start:	 Loading data
2017-09-22 14:44:04,179 DEBUG: Start:	 Loading data
2017-09-22 14:44:04,193 DEBUG: Done:	 Loading data
2017-09-22 14:44:04,193 DEBUG: Done:	 Loading data
2017-09-22 14:44:04,193 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : Adaboost
2017-09-22 14:44:04,193 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : DecisionTree
2017-09-22 14:44:04,194 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:04,194 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:04,240 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:04,240 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:04,240 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:04,241 DEBUG: Start:	 RandomSearch best settings with 2 iterations for DecisionTree
2017-09-22 14:44:04,244 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:04,244 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:04,244 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:04,244 DEBUG: Start:	 RandomSearch best settings with 2 iterations for Adaboost
2017-09-22 14:44:06,280 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:06,281 DEBUG: Start:	 Training
2017-09-22 14:44:06,785 DEBUG: Done:	 Training
2017-09-22 14:44:06,785 DEBUG: Start:	 Predicting
2017-09-22 14:44:06,797 DEBUG: Done:	 Predicting
2017-09-22 14:44:06,797 DEBUG: Info:	 Time for training and predicting: 2.61771392822[s]
2017-09-22 14:44:06,797 DEBUG: Start:	 Getting Results
2017-09-22 14:44:06,826 DEBUG: Done:	 Getting Results
2017-09-22 14:44:06,827 INFO: Classification on awaexp database for cq-hist with DecisionTree, and 2 statistical iterations

accuracy_score on train : 0.879895561358, with STD : 0.0
accuracy_score on test : 0.711656441718, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- Decision Tree with max_depth : 5
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.879895561358
		- Score on test : 0.711656441718
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.877005347594
		- Score on test : 0.694805194805
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.877005347594
		- Score on test : 0.694805194805
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.120104438642
		- Score on test : 0.288343558282
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.879895561358
		- Score on test : 0.711656441718
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.760631611356
		- Score on test : 0.425917811428
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.898630136986
		- Score on test : 0.737931034483
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.856396866841
		- Score on test : 0.656441717791
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.879895561358
		- Score on test : 0.711656441718
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.120104438642
		- Score on test : 0.288343558282


 Classification took 0:00:02
2017-09-22 14:44:06,827 INFO: Done:	 Result Analysis
2017-09-22 14:44:06,921 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:06,921 DEBUG: Start:	 Training
2017-09-22 14:44:07,795 DEBUG: Done:	 Training
2017-09-22 14:44:07,795 DEBUG: Start:	 Predicting
2017-09-22 14:44:07,811 DEBUG: Done:	 Predicting
2017-09-22 14:44:07,811 DEBUG: Info:	 Time for training and predicting: 3.63157486916[s]
2017-09-22 14:44:07,811 DEBUG: Start:	 Getting Results
2017-09-22 14:44:07,839 DEBUG: Done:	 Getting Results
2017-09-22 14:44:07,839 INFO: Classification on awaexp database for cq-hist with Adaboost, and 2 statistical iterations

accuracy_score on train : 1.0, with STD : 0.0
accuracy_score on test : 0.647239263804, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- Adaboost with num_esimators : 5, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.647239263804
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.65671641791
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.65671641791
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.352760736196
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.647239263804
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.294928439892
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.639534883721
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.674846625767
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.647239263804
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.352760736196


 Classification took 0:00:03
2017-09-22 14:44:07,861 INFO: Done:	 Result Analysis
2017-09-22 14:44:07,944 DEBUG: Start:	 Loading data
2017-09-22 14:44:07,944 DEBUG: Start:	 Loading data
2017-09-22 14:44:07,957 DEBUG: Done:	 Loading data
2017-09-22 14:44:07,957 DEBUG: Done:	 Loading data
2017-09-22 14:44:07,957 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : KNN
2017-09-22 14:44:07,957 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : RandomForest
2017-09-22 14:44:07,958 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:07,958 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:07,993 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:07,994 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:07,994 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:07,994 DEBUG: Start:	 RandomSearch best settings with 2 iterations for KNN
2017-09-22 14:44:08,001 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:08,001 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:08,001 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:08,001 DEBUG: Start:	 RandomSearch best settings with 2 iterations for RandomForest
2017-09-22 14:44:08,585 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:08,585 DEBUG: Start:	 Training
2017-09-22 14:44:08,765 DEBUG: Done:	 Training
2017-09-22 14:44:08,765 DEBUG: Start:	 Predicting
2017-09-22 14:44:08,820 DEBUG: Done:	 Predicting
2017-09-22 14:44:08,820 DEBUG: Info:	 Time for training and predicting: 0.8758289814[s]
2017-09-22 14:44:08,820 DEBUG: Start:	 Getting Results
2017-09-22 14:44:08,849 DEBUG: Done:	 Getting Results
2017-09-22 14:44:08,849 INFO: Classification on awaexp database for cq-hist with RandomForest, and 2 statistical iterations

accuracy_score on train : 0.88772845953, with STD : 0.0
accuracy_score on test : 0.699386503067, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- Random Forest with num_esimators : 17, max_depth : 5
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.88772845953
		- Score on test : 0.699386503067
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.883783783784
		- Score on test : 0.675496688742
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.883783783784
		- Score on test : 0.675496688742
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.11227154047
		- Score on test : 0.300613496933
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.88772845953
		- Score on test : 0.699386503067
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.777249922224
		- Score on test : 0.403167163571
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.915966386555
		- Score on test : 0.73381294964
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.853785900783
		- Score on test : 0.625766871166
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.88772845953
		- Score on test : 0.699386503067
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.11227154047
		- Score on test : 0.300613496933


 Classification took 0:00:00
2017-09-22 14:44:08,849 INFO: Done:	 Result Analysis
2017-09-22 14:44:10,585 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:10,585 DEBUG: Start:	 Training
2017-09-22 14:44:10,632 DEBUG: Done:	 Training
2017-09-22 14:44:10,632 DEBUG: Start:	 Predicting
2017-09-22 14:44:17,801 DEBUG: Done:	 Predicting
2017-09-22 14:44:17,801 DEBUG: Info:	 Time for training and predicting: 9.85622215271[s]
2017-09-22 14:44:17,801 DEBUG: Start:	 Getting Results
2017-09-22 14:44:17,828 DEBUG: Done:	 Getting Results
2017-09-22 14:44:17,828 INFO: Classification on awaexp database for cq-hist with KNN, and 2 statistical iterations

accuracy_score on train : 0.707571801567, with STD : 0.0
accuracy_score on test : 0.634969325153, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 17
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.707571801567
		- Score on test : 0.634969325153
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.742528735632
		- Score on test : 0.687664041995
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.742528735632
		- Score on test : 0.687664041995
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.292428198433
		- Score on test : 0.365030674847
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.707571801567
		- Score on test : 0.634969325153
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.431350734688
		- Score on test : 0.286756025235
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.663244353183
		- Score on test : 0.600917431193
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.843342036554
		- Score on test : 0.803680981595
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.707571801567
		- Score on test : 0.634969325153
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.292428198433
		- Score on test : 0.365030674847


 Classification took 0:00:09
2017-09-22 14:44:17,828 INFO: Done:	 Result Analysis
2017-09-22 14:44:17,931 DEBUG: Start:	 Loading data
2017-09-22 14:44:17,931 DEBUG: Start:	 Loading data
2017-09-22 14:44:17,945 DEBUG: Done:	 Loading data
2017-09-22 14:44:17,945 DEBUG: Done:	 Loading data
2017-09-22 14:44:17,945 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SGD
2017-09-22 14:44:17,945 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SVMLinear
2017-09-22 14:44:17,946 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:17,946 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:17,975 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:17,975 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:17,975 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:17,975 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:17,975 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:17,975 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:17,975 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SGD
2017-09-22 14:44:17,975 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SVMLinear
2017-09-22 14:44:18,645 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:18,645 DEBUG: Start:	 Training
2017-09-22 14:44:18,706 DEBUG: Done:	 Training
2017-09-22 14:44:18,706 DEBUG: Start:	 Predicting
2017-09-22 14:44:18,718 DEBUG: Done:	 Predicting
2017-09-22 14:44:18,718 DEBUG: Info:	 Time for training and predicting: 0.786246061325[s]
2017-09-22 14:44:18,718 DEBUG: Start:	 Getting Results
2017-09-22 14:44:18,760 DEBUG: Done:	 Getting Results
2017-09-22 14:44:18,760 INFO: Classification on awaexp database for cq-hist with SGD, and 2 statistical iterations

accuracy_score on train : 0.664490861619, with STD : 0.0
accuracy_score on test : 0.70245398773, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SGDClassifier with loss : modified_huber, penalty : l2
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.664490861619
		- Score on test : 0.70245398773
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.655957161981
		- Score on test : 0.701538461538
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.655957161981
		- Score on test : 0.701538461538
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.335509138381
		- Score on test : 0.29754601227
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.664490861619
		- Score on test : 0.70245398773
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.329387282132
		- Score on test : 0.404915595608
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.673076923077
		- Score on test : 0.703703703704
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.639686684073
		- Score on test : 0.699386503067
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.664490861619
		- Score on test : 0.70245398773
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.335509138381
		- Score on test : 0.29754601227


 Classification took 0:00:00
2017-09-22 14:44:18,761 INFO: Done:	 Result Analysis
2017-09-22 14:44:21,563 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:21,563 DEBUG: Start:	 Training
2017-09-22 14:44:27,074 DEBUG: Done:	 Training
2017-09-22 14:44:27,074 DEBUG: Start:	 Predicting
2017-09-22 14:44:29,936 DEBUG: Done:	 Predicting
2017-09-22 14:44:29,936 DEBUG: Info:	 Time for training and predicting: 12.0045089722[s]
2017-09-22 14:44:29,936 DEBUG: Start:	 Getting Results
2017-09-22 14:44:29,963 DEBUG: Done:	 Getting Results
2017-09-22 14:44:29,963 INFO: Classification on awaexp database for cq-hist with SVMLinear, and 2 statistical iterations

accuracy_score on train : 1.0, with STD : 0.0
accuracy_score on test : 0.653374233129, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SVM Linear with C : 3089
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.653374233129
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.676217765043
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.676217765043
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.346625766871
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.653374233129
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.30984858301
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.634408602151
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.723926380368
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.653374233129
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.346625766871


 Classification took 0:00:12
2017-09-22 14:44:29,964 INFO: Done:	 Result Analysis
2017-09-22 14:44:30,120 DEBUG: Start:	 Loading data
2017-09-22 14:44:30,120 DEBUG: Start:	 Loading data
2017-09-22 14:44:30,134 DEBUG: Done:	 Loading data
2017-09-22 14:44:30,134 DEBUG: Done:	 Loading data
2017-09-22 14:44:30,134 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SVMRBF
2017-09-22 14:44:30,134 DEBUG: Info:	 Classification - Database:awaexp Feature:cq-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SVMPoly
2017-09-22 14:44:30,135 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:30,135 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:30,164 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:30,164 DEBUG: Info:	 Shape X_train:(766, 2688), Length of y_train:766
2017-09-22 14:44:30,164 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:30,164 DEBUG: Info:	 Shape X_test:(326, 2688), Length of y_test:326
2017-09-22 14:44:30,164 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:30,164 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:30,165 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SVMPoly
2017-09-22 14:44:30,165 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SVMRBF
2017-09-22 14:44:35,098 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:35,098 DEBUG: Start:	 Training
2017-09-22 14:44:36,476 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:36,476 DEBUG: Start:	 Training
2017-09-22 14:44:42,311 DEBUG: Done:	 Training
2017-09-22 14:44:42,311 DEBUG: Start:	 Predicting
2017-09-22 14:44:46,225 DEBUG: Done:	 Training
2017-09-22 14:44:46,225 DEBUG: Start:	 Predicting
2017-09-22 14:44:46,423 DEBUG: Done:	 Predicting
2017-09-22 14:44:46,423 DEBUG: Info:	 Time for training and predicting: 16.3026628494[s]
2017-09-22 14:44:46,423 DEBUG: Start:	 Getting Results
2017-09-22 14:44:46,468 DEBUG: Done:	 Getting Results
2017-09-22 14:44:46,468 INFO: Classification on awaexp database for cq-hist with SVMRBF, and 2 statistical iterations

accuracy_score on train : 0.929503916449, with STD : 0.0
accuracy_score on test : 0.708588957055, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SVM RBF with C : 3089
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.929503916449
		- Score on test : 0.708588957055
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.9296875
		- Score on test : 0.727793696275
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.9296875
		- Score on test : 0.727793696275
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0704960835509
		- Score on test : 0.291411042945
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.929503916449
		- Score on test : 0.708588957055
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.859019545097
		- Score on test : 0.421394072893
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.927272727273
		- Score on test : 0.682795698925
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.932114882507
		- Score on test : 0.779141104294
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.929503916449
		- Score on test : 0.708588957055
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0704960835509
		- Score on test : 0.291411042945


 Classification took 0:00:16
2017-09-22 14:44:46,469 INFO: Done:	 Result Analysis
2017-09-22 14:44:51,890 DEBUG: Done:	 Predicting
2017-09-22 14:44:51,890 DEBUG: Info:	 Time for training and predicting: 21.7691369057[s]
2017-09-22 14:44:51,890 DEBUG: Start:	 Getting Results
2017-09-22 14:44:51,917 DEBUG: Done:	 Getting Results
2017-09-22 14:44:51,917 INFO: Classification on awaexp database for cq-hist with SVMPoly, and 2 statistical iterations

accuracy_score on train : 0.936031331593, with STD : 0.0
accuracy_score on test : 0.564417177914, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : cq-hist	 View shape : (1092, 2688)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SVM Poly with C : 3089
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.936031331593
		- Score on test : 0.564417177914
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.932784636488
		- Score on test : 0.403361344538
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.932784636488
		- Score on test : 0.403361344538
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0639686684073
		- Score on test : 0.435582822086
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.936031331593
		- Score on test : 0.564417177914
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.8761607063
		- Score on test : 0.153056508587
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.982658959538
		- Score on test : 0.64
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.88772845953
		- Score on test : 0.294478527607
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.936031331593
		- Score on test : 0.564417177914
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0639686684073
		- Score on test : 0.435582822086


 Classification took 0:00:21
2017-09-22 14:44:51,917 INFO: Done:	 Result Analysis
2017-09-22 14:44:52,016 DEBUG: Start:	 Loading data
2017-09-22 14:44:52,017 DEBUG: Start:	 Loading data
2017-09-22 14:44:52,028 DEBUG: Done:	 Loading data
2017-09-22 14:44:52,028 DEBUG: Done:	 Loading data
2017-09-22 14:44:52,028 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : Adaboost
2017-09-22 14:44:52,028 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : DecisionTree
2017-09-22 14:44:52,029 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:52,029 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:52,057 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:44:52,057 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:44:52,057 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:44:52,057 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:52,057 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:44:52,058 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:52,058 DEBUG: Start:	 RandomSearch best settings with 2 iterations for Adaboost
2017-09-22 14:44:52,058 DEBUG: Start:	 RandomSearch best settings with 2 iterations for DecisionTree
2017-09-22 14:44:53,296 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:53,296 DEBUG: Start:	 Training
2017-09-22 14:44:53,647 DEBUG: Done:	 Training
2017-09-22 14:44:53,647 DEBUG: Start:	 Predicting
2017-09-22 14:44:53,659 DEBUG: Done:	 Predicting
2017-09-22 14:44:53,660 DEBUG: Info:	 Time for training and predicting: 1.64258408546[s]
2017-09-22 14:44:53,660 DEBUG: Start:	 Getting Results
2017-09-22 14:44:53,701 DEBUG: Done:	 Getting Results
2017-09-22 14:44:53,701 INFO: Classification on awaexp database for lss-hist with DecisionTree, and 2 statistical iterations

accuracy_score on train : 0.8772845953, with STD : 0.0
accuracy_score on test : 0.684049079755, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- Decision Tree with max_depth : 5
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.8772845953
		- Score on test : 0.684049079755
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.880407124682
		- Score on test : 0.692537313433
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.880407124682
		- Score on test : 0.692537313433
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.1227154047
		- Score on test : 0.315950920245
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.8772845953
		- Score on test : 0.684049079755
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.755600100768
		- Score on test : 0.368660549865
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.858560794045
		- Score on test : 0.674418604651
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.903394255875
		- Score on test : 0.711656441718
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.8772845953
		- Score on test : 0.684049079755
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.1227154047
		- Score on test : 0.315950920245


 Classification took 0:00:01
2017-09-22 14:44:53,701 INFO: Done:	 Result Analysis
2017-09-22 14:44:53,730 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:53,730 DEBUG: Start:	 Training
2017-09-22 14:44:54,116 DEBUG: Done:	 Training
2017-09-22 14:44:54,116 DEBUG: Start:	 Predicting
2017-09-22 14:44:54,129 DEBUG: Done:	 Predicting
2017-09-22 14:44:54,129 DEBUG: Info:	 Time for training and predicting: 2.11217498779[s]
2017-09-22 14:44:54,129 DEBUG: Start:	 Getting Results
2017-09-22 14:44:54,156 DEBUG: Done:	 Getting Results
2017-09-22 14:44:54,156 INFO: Classification on awaexp database for lss-hist with Adaboost, and 2 statistical iterations

accuracy_score on train : 1.0, with STD : 0.0
accuracy_score on test : 0.662576687117, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- Adaboost with num_esimators : 3, base_estimators : DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.662576687117
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.664634146341
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.664634146341
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.337423312883
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.662576687117
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.325177853144
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.660606060606
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.668711656442
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.662576687117
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.337423312883


 Classification took 0:00:02
2017-09-22 14:44:54,156 INFO: Done:	 Result Analysis
2017-09-22 14:44:54,277 DEBUG: Start:	 Loading data
2017-09-22 14:44:54,277 DEBUG: Start:	 Loading data
2017-09-22 14:44:54,288 DEBUG: Done:	 Loading data
2017-09-22 14:44:54,288 DEBUG: Done:	 Loading data
2017-09-22 14:44:54,288 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : KNN
2017-09-22 14:44:54,288 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : RandomForest
2017-09-22 14:44:54,289 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:54,289 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:44:54,317 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:44:54,317 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:44:54,317 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:44:54,317 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:44:54,317 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:54,317 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:44:54,317 DEBUG: Start:	 RandomSearch best settings with 2 iterations for KNN
2017-09-22 14:44:54,317 DEBUG: Start:	 RandomSearch best settings with 2 iterations for RandomForest
2017-09-22 14:44:54,801 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:54,801 DEBUG: Start:	 Training
2017-09-22 14:44:54,945 DEBUG: Done:	 Training
2017-09-22 14:44:54,945 DEBUG: Start:	 Predicting
2017-09-22 14:44:54,997 DEBUG: Done:	 Predicting
2017-09-22 14:44:54,998 DEBUG: Info:	 Time for training and predicting: 0.719907999039[s]
2017-09-22 14:44:54,998 DEBUG: Start:	 Getting Results
2017-09-22 14:44:55,026 DEBUG: Done:	 Getting Results
2017-09-22 14:44:55,026 INFO: Classification on awaexp database for lss-hist with RandomForest, and 2 statistical iterations

accuracy_score on train : 0.89817232376, with STD : 0.0
accuracy_score on test : 0.726993865031, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- Random Forest with num_esimators : 17, max_depth : 5
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.89817232376
		- Score on test : 0.726993865031
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.896551724138
		- Score on test : 0.702341137124
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.896551724138
		- Score on test : 0.702341137124
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.10182767624
		- Score on test : 0.273006134969
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.89817232376
		- Score on test : 0.726993865031
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.796735808844
		- Score on test : 0.460347156659
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.911051212938
		- Score on test : 0.772058823529
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.882506527415
		- Score on test : 0.644171779141
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.89817232376
		- Score on test : 0.726993865031
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.10182767624
		- Score on test : 0.273006134969


 Classification took 0:00:00
2017-09-22 14:44:55,027 INFO: Done:	 Result Analysis
2017-09-22 14:44:56,175 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:44:56,175 DEBUG: Start:	 Training
2017-09-22 14:44:56,204 DEBUG: Done:	 Training
2017-09-22 14:44:56,205 DEBUG: Start:	 Predicting
2017-09-22 14:45:01,385 DEBUG: Done:	 Predicting
2017-09-22 14:45:01,385 DEBUG: Info:	 Time for training and predicting: 7.10769701004[s]
2017-09-22 14:45:01,385 DEBUG: Start:	 Getting Results
2017-09-22 14:45:01,423 DEBUG: Done:	 Getting Results
2017-09-22 14:45:01,423 INFO: Classification on awaexp database for lss-hist with KNN, and 2 statistical iterations

accuracy_score on train : 0.725848563969, with STD : 0.0
accuracy_score on test : 0.628834355828, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- K nearest Neighbors with  n_neighbors: 17
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.725848563969
		- Score on test : 0.628834355828
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.6875
		- Score on test : 0.563176895307
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.6875
		- Score on test : 0.563176895307
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.274151436031
		- Score on test : 0.371165644172
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.725848563969
		- Score on test : 0.628834355828
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.465948579669
		- Score on test : 0.270164906057
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.799307958478
		- Score on test : 0.684210526316
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.603133159269
		- Score on test : 0.478527607362
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.725848563969
		- Score on test : 0.628834355828
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.274151436031
		- Score on test : 0.371165644172


 Classification took 0:00:07
2017-09-22 14:45:01,423 INFO: Done:	 Result Analysis
2017-09-22 14:45:01,546 DEBUG: Start:	 Loading data
2017-09-22 14:45:01,547 DEBUG: Start:	 Loading data
2017-09-22 14:45:01,554 DEBUG: Done:	 Loading data
2017-09-22 14:45:01,554 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SGD
2017-09-22 14:45:01,554 DEBUG: Done:	 Loading data
2017-09-22 14:45:01,555 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:45:01,555 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SVMLinear
2017-09-22 14:45:01,555 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:45:01,572 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:45:01,572 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:45:01,572 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:45:01,572 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SGD
2017-09-22 14:45:01,573 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:45:01,573 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:45:01,573 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:45:01,573 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SVMLinear
2017-09-22 14:45:01,797 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:45:01,797 DEBUG: Start:	 Training
2017-09-22 14:45:01,821 DEBUG: Done:	 Training
2017-09-22 14:45:01,822 DEBUG: Start:	 Predicting
2017-09-22 14:45:01,830 DEBUG: Done:	 Predicting
2017-09-22 14:45:01,831 DEBUG: Info:	 Time for training and predicting: 0.283898830414[s]
2017-09-22 14:45:01,831 DEBUG: Start:	 Getting Results
2017-09-22 14:45:01,871 DEBUG: Done:	 Getting Results
2017-09-22 14:45:01,871 INFO: Classification on awaexp database for lss-hist with SGD, and 2 statistical iterations

accuracy_score on train : 0.869451697128, with STD : 0.0
accuracy_score on test : 0.739263803681, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SGDClassifier with loss : modified_huber, penalty : l2
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.869451697128
		- Score on test : 0.739263803681
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.850299401198
		- Score on test : 0.681647940075
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.850299401198
		- Score on test : 0.681647940075
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.130548302872
		- Score on test : 0.260736196319
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.869451697128
		- Score on test : 0.739263803681
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.764348587476
		- Score on test : 0.51333567333
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.99649122807
		- Score on test : 0.875
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.741514360313
		- Score on test : 0.558282208589
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.869451697128
		- Score on test : 0.739263803681
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.130548302872
		- Score on test : 0.260736196319


 Classification took 0:00:00
2017-09-22 14:45:01,872 INFO: Done:	 Result Analysis
2017-09-22 14:45:03,966 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:45:03,967 DEBUG: Start:	 Training
2017-09-22 14:45:07,715 DEBUG: Done:	 Training
2017-09-22 14:45:07,715 DEBUG: Start:	 Predicting
2017-09-22 14:45:09,521 DEBUG: Done:	 Predicting
2017-09-22 14:45:09,522 DEBUG: Info:	 Time for training and predicting: 7.97445011139[s]
2017-09-22 14:45:09,522 DEBUG: Start:	 Getting Results
2017-09-22 14:45:09,549 DEBUG: Done:	 Getting Results
2017-09-22 14:45:09,549 INFO: Classification on awaexp database for lss-hist with SVMLinear, and 2 statistical iterations

accuracy_score on train : 1.0, with STD : 0.0
accuracy_score on test : 0.782208588957, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SVM Linear with C : 3089
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.782208588957
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.781538461538
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.781538461538
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.217791411043
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.782208588957
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.564427799938
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.783950617284
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.779141104294
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.782208588957
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.217791411043


 Classification took 0:00:07
2017-09-22 14:45:09,549 INFO: Done:	 Result Analysis
2017-09-22 14:45:09,620 DEBUG: Start:	 Loading data
2017-09-22 14:45:09,620 DEBUG: Start:	 Loading data
2017-09-22 14:45:09,628 DEBUG: Done:	 Loading data
2017-09-22 14:45:09,628 DEBUG: Done:	 Loading data
2017-09-22 14:45:09,628 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SVMRBF
2017-09-22 14:45:09,628 DEBUG: Info:	 Classification - Database:awaexp Feature:lss-hist train_size:0.7, CrossValidation k-folds:2, cores:1, algorithm : SVMPoly
2017-09-22 14:45:09,629 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:45:09,629 DEBUG: Start:	 Determine Train/Test split for iteration 1
2017-09-22 14:45:09,647 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:45:09,647 DEBUG: Info:	 Shape X_train:(766, 2000), Length of y_train:766
2017-09-22 14:45:09,647 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:45:09,647 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:45:09,647 DEBUG: Info:	 Shape X_test:(326, 2000), Length of y_test:326
2017-09-22 14:45:09,647 DEBUG: Done:	 Determine Train/Test split
2017-09-22 14:45:09,647 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SVMRBF
2017-09-22 14:45:09,647 DEBUG: Start:	 RandomSearch best settings with 2 iterations for SVMPoly
2017-09-22 14:45:11,911 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:45:11,912 DEBUG: Start:	 Training
2017-09-22 14:45:13,964 DEBUG: Done:	 RandomSearch best settings
2017-09-22 14:45:13,964 DEBUG: Start:	 Training
2017-09-22 14:45:15,835 DEBUG: Done:	 Training
2017-09-22 14:45:15,835 DEBUG: Start:	 Predicting
2017-09-22 14:45:17,760 DEBUG: Done:	 Predicting
2017-09-22 14:45:17,760 DEBUG: Info:	 Time for training and predicting: 8.14053297043[s]
2017-09-22 14:45:17,761 DEBUG: Start:	 Getting Results
2017-09-22 14:45:17,789 DEBUG: Done:	 Getting Results
2017-09-22 14:45:17,790 INFO: Classification on awaexp database for lss-hist with SVMPoly, and 2 statistical iterations

accuracy_score on train : 0.530026109661, with STD : 0.0
accuracy_score on test : 0.457055214724, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SVM Poly with C : 3089
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 0.530026109661
		- Score on test : 0.457055214724
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.669724770642
		- Score on test : 0.598639455782
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 0.669724770642
		- Score on test : 0.598639455782
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.469973890339
		- Score on test : 0.542944785276
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 0.530026109661
		- Score on test : 0.457055214724
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 0.112613932219
		- Score on test : -0.121195088186
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.516265912306
		- Score on test : 0.474820143885
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 0.953002610966
		- Score on test : 0.80981595092
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 0.530026109661
		- Score on test : 0.457055214724
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.469973890339
		- Score on test : 0.542944785276


 Classification took 0:00:08
2017-09-22 14:45:17,790 INFO: Done:	 Result Analysis
2017-09-22 14:45:20,806 DEBUG: Done:	 Training
2017-09-22 14:45:20,806 DEBUG: Start:	 Predicting
2017-09-22 14:45:24,334 DEBUG: Done:	 Predicting
2017-09-22 14:45:24,335 DEBUG: Info:	 Time for training and predicting: 14.7147290707[s]
2017-09-22 14:45:24,335 DEBUG: Start:	 Getting Results
2017-09-22 14:45:24,361 DEBUG: Done:	 Getting Results
2017-09-22 14:45:24,361 INFO: Classification on awaexp database for lss-hist with SVMRBF, and 2 statistical iterations

accuracy_score on train : 1.0, with STD : 0.0
accuracy_score on test : 0.524539877301, with STD : 0.0

Database configuration : 
	- Database name : awaexp
	- View name : lss-hist	 View shape : (1092, 2000)
	- Learning Rate : 0.7
	- Labels used : 
	- Number of cross validation folds : 2

Classifier configuration : 
	- SVM RBF with C : 3089
	- Executed on 1 core(s) 
	- Got configuration using randomized search with 2 iterations 


	For Accuracy score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.524539877301
	For F1 score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.670912951168
	For F-beta score using None as sample_weights, None as labels, 1 as pos_label, binary as average, 1.0 as beta (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.670912951168
	For Hamming loss using None as classes (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.475460122699
	For Jaccard similarity score using None as sample_weights (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.524539877301
	For Matthews correlation coefficient (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.10744306187
	For Precision score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.512987012987
	For Recall score using None as sample_weights, None as labels, 1 as pos_label, binary as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.969325153374
	For ROC AUC score using None as sample_weights, micro as average (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.524539877301
	For Zero one loss using None as sample_weights (lower is better) : 
		- Score on train : 0.0
		- Score on test : 0.475460122699


 Classification took 0:00:14
2017-09-22 14:45:24,361 INFO: Done:	 Result Analysis
2017-09-22 14:45:24,514 INFO: ### Main Programm for Multiview Classification
2017-09-22 14:45:24,514 INFO: ### Classification - Database : awaexp ; Views : cq-hist, lss-hist ; Algorithm : Fusion ; Cores : 1
2017-09-22 14:45:24,515 INFO: ### Main Programm for Multiview Classification
2017-09-22 14:45:24,515 INFO: ### Classification - Database : awaexp ; Views : cq-hist, lss-hist ; Algorithm : Fusion ; Cores : 1
2017-09-22 14:45:24,515 INFO: Info:	 Shape of cq-hist :(1092, 2688)
2017-09-22 14:45:24,516 INFO: Info:	 Shape of cq-hist :(1092, 2688)
2017-09-22 14:45:24,516 INFO: Info:	 Shape of lss-hist :(1092, 2000)
2017-09-22 14:45:24,516 INFO: Done:	 Read Database Files
2017-09-22 14:45:24,516 INFO: Start:	 Determine validation split for ratio 0.7
2017-09-22 14:45:24,517 INFO: Info:	 Shape of lss-hist :(1092, 2000)
2017-09-22 14:45:24,517 INFO: Done:	 Read Database Files
2017-09-22 14:45:24,517 INFO: Start:	 Determine validation split for ratio 0.7
